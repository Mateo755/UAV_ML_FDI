{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mateo755/UAV_ML_FDI/blob/main/FDI_UAV_W%26B_(Colab).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCI_N8xy07lW"
      },
      "source": [
        "# UAV Propeller Fault Detection System (Parrot Bebop 2)\n",
        "\n",
        "This project focuses on the development and validation of fault detection and isolation (FDI) methods for the propulsion system of the **Parrot Bebop 2** unmanned aerial vehicle (UAV). The analysis utilizes inertial sensor data (accelerometer and gyroscope) collected during real-world flight experiments.\n",
        "\n",
        "### Research Problem\n",
        "The primary objective is to classify the technical state of the propellers based on vibration signals. We analyze various fault scenarios across four rotors (A, B, C, D), distinguishing between nominal states and specific defects such as chipped edges or bent blades.\n",
        "\n",
        "### Methodology\n",
        "This notebook compares two signal processing approaches:\n",
        "1.  **Time Domain Analysis**\n",
        "2.  **Frequency Domain Analysis**\n",
        "\n",
        "Experiment tracking and performance visualization are managed via **Weights & Biases (W&B)**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Do56U9O8568"
      },
      "source": [
        "# 1. Environment Setup & Global Configuration\n",
        "Installation of necessary libraries (Weights & Biases for experiment tracking) and importing standard data science modules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHNOxhxQz5nx"
      },
      "outputs": [],
      "source": [
        "!pip install wandb -q\n",
        "import wandb\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report, precision_score, \\\n",
        "                            recall_score, f1_score, roc_curve, auc, ConfusionMatrixDisplay\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from tensorflow.keras import layers, models, regularizers\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from wandb.integration.keras import WandbMetricsLogger, WandbModelCheckpoint\n",
        "\n",
        "# Login to W&B\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoUfiIkK1LyI"
      },
      "source": [
        "# 2. Exploratory Data Analysis\n",
        "Initial inspection of the accelerometer and gyroscope data from the UAV. We examine the column structure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5cYD7jd2J0R"
      },
      "source": [
        "## 2.1 Time domain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EfFehWg_BIQW"
      },
      "outputs": [],
      "source": [
        "!unzip -q /content/Normalized_data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUIOpXWM1LhW"
      },
      "outputs": [],
      "source": [
        "df_time = pd.read_csv('/content/Normalized_data/Bebop2_16g_1kdps_normalized_0000.csv')\n",
        "df_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXXPsSeP1cKs"
      },
      "outputs": [],
      "source": [
        "df_time.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzQLUTk82LtL"
      },
      "source": [
        "## 2.2 Frequency Domain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2ARDYyABHWn"
      },
      "outputs": [],
      "source": [
        "!unzip -q /content/FFT_data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWm9whI32SFf"
      },
      "outputs": [],
      "source": [
        "df_freq = pd.read_csv('/content/FFT_data/128_Hann_20_52/Bebop2_16g_FFT_ACCEL_128_Hann_20_52_0000.csv', header=None)\n",
        "df_freq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_cnT-vQb27cS"
      },
      "outputs": [],
      "source": [
        "df_freq.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpQQlUm49FYy"
      },
      "source": [
        "# 3. Fault Scenario Mapping\n",
        "\n",
        "The Bebop 2 flight data is labeled using a 4-digit code (e.g., `1022`), defining the state of each propeller (A, B, C, D):\n",
        "* **0**: Nominal (Functional propeller).\n",
        "* **1**: Fault Type I (e.g., chipped edge).\n",
        "* **2**: Fault Type II (e.g., bent tip/severe damage).\n",
        "\n",
        "Below, we define the mapping of these physical scenarios to model class labels. Depending on the diagnostic granularity required, the problem can be framed as a **5-class problem** (aggregated by the number of faults) or a **20-class problem** (precise fault configuration)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPL1aCh09auz"
      },
      "outputs": [],
      "source": [
        "# Select Experiment Mode\n",
        "CLASS_MODE = \"20class\"   # Options: \"5class\" (aggregated) or \"20class\" (precise diagnosis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9yIykYh9RcZ"
      },
      "outputs": [],
      "source": [
        "# Precise mapping for 20 unique scenarios (Parrot Bebop 2)\n",
        "scenario_to_class_20 = {\n",
        "    \"0000\": 0,  # Nominal state\n",
        "    \"1000\": 1, \"0100\": 2, \"0010\": 3, \"0001\": 4,     # Single faults (Type 1)\n",
        "    \"2000\": 5, \"0200\": 6, \"0020\": 7, \"0002\": 8,     # Single faults (Type 2)\n",
        "    \"1100\": 9, \"1020\": 10, \"1002\": 11, \"0120\": 12, \"0102\": 13, \"0022\": 14, # Dual faults\n",
        "    \"1120\": 15, \"1102\": 16, \"1022\": 17, \"0122\": 18, # Triple faults\n",
        "    \"1122\": 19, # All propellers faulty\n",
        "}\n",
        "\n",
        "# Simplified mapping (Number of faulty rotors)\n",
        "scenario_to_class_5 = {\n",
        "    \"0000\": 0,\n",
        "    \"1000\": 1, \"0100\": 1, \"0010\": 1, \"0001\": 1,\n",
        "    \"2000\": 1, \"0200\": 1, \"0020\": 1, \"0002\": 1,\n",
        "    \"1100\": 2, \"1020\": 2, \"1002\": 2, \"0120\": 2, \"0102\": 2, \"0022\": 2,\n",
        "    \"1120\": 3, \"1102\": 3, \"1022\": 3, \"0122\": 3,\n",
        "    \"1122\": 4,\n",
        "}\n",
        "\n",
        "if CLASS_MODE == \"5class\":\n",
        "    scenario_to_class = scenario_to_class_5\n",
        "elif CLASS_MODE == \"20class\":\n",
        "    scenario_to_class = scenario_to_class_20\n",
        "else:\n",
        "    raise ValueError(\"Invalid CLASS_MODE selected.\")\n",
        "\n",
        "NUM_CLASSES = len(set(scenario_to_class.values()))\n",
        "print(f\"Experiment Mode: {CLASS_MODE} | Total Classes: {NUM_CLASSES}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTBVnJlJ-oN6"
      },
      "source": [
        "# 3. Time Domain Analysis, data preparation\n",
        "\n",
        "In this section, we process normalized time-series signals from the accelerometers and gyroscopes. Since the data represents a continuous flight stream, we apply a **sliding window** technique to segment the signal into fixed-length samples (e.g., 256 measurement points).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aovzZqvy-8L9"
      },
      "source": [
        "## 3.1 Data Segmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRPd81ZC-nf7"
      },
      "outputs": [],
      "source": [
        "# Path to normalized time-domain data\n",
        "DATA_DIR_TIME = r\"/content/Normalized_data\"\n",
        "SAMPLE_SIZE = 8             # Window length\n",
        "N_FEATURES = 24               # Input channels (e.g., 3-axis accel + 3-axis gyro per sensor)\n",
        "SENSOR_MODE = \"both\"          # \"accel\" + \"gyro\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ho9zzXAMDFeG"
      },
      "outputs": [],
      "source": [
        "def make_windows_from_df(df: pd.DataFrame, sample_size: int):\n",
        "    \"\"\"\n",
        "    Segments time-series data into non-overlapping windows.\n",
        "    \"\"\"\n",
        "    data = df.values.astype(\"float32\")\n",
        "    n_total = len(data)\n",
        "    n_windows = n_total // sample_size\n",
        "    if n_windows == 0:\n",
        "        return np.empty((0, sample_size, data.shape[1]), dtype=\"float32\")\n",
        "    data = data[:n_windows * sample_size]\n",
        "    windows = data.reshape(n_windows, sample_size, data.shape[1])\n",
        "    return windows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URJwy-VoYAcQ"
      },
      "source": [
        "### Processing Pipeline\n",
        "The following loop iterates through all normalized CSV files in time domain. For each file, it:\n",
        "1.  Extracts the scenario code (e.g., `0000`) from the filename.\n",
        "2.  Checks if the scenario exists in our defined class mapping.\n",
        "3.  Loads the data and applies the sliding window segmentation.\n",
        "4.  Accumulates the processed windows (`X`) and corresponding labels (`y`) into a single dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gl0X-52e_V-k"
      },
      "outputs": [],
      "source": [
        "# Data pass through window function\n",
        "\n",
        "X_list = []\n",
        "y_list = []\n",
        "\n",
        "norm_data_files_pattern = os.path.join(DATA_DIR_TIME, \"Bebop2_16g_1kdps_normalized_*.csv\")\n",
        "\n",
        "for path in glob.glob(norm_data_files_pattern):\n",
        "    fname = os.path.basename(path)\n",
        "    # ostatni fragment po \"_\" to kod scenariusza, np. \"0000\"\n",
        "    scenario = os.path.splitext(fname)[0].split(\"_\")[-1]\n",
        "\n",
        "    if scenario not in scenario_to_class:\n",
        "        print(f\"Pomijam {fname} – scenariusz {scenario} nie jest w mapowaniu.\")\n",
        "        continue\n",
        "\n",
        "    label = scenario_to_class[scenario]\n",
        "\n",
        "    df = pd.read_csv(path)\n",
        "\n",
        "    # jeżeli w pliku są inne kolumny niż 24 sensory, tu można wybrać tylko potrzebne:\n",
        "    # df = df[[\"A_aX\",\"A_aY\",...,\"D_gZ\"]]\n",
        "\n",
        "    windows = make_windows_from_df(df, SAMPLE_SIZE)\n",
        "    if windows.shape[0] == 0:\n",
        "        print(f\"Za mało danych w {fname} na choć jedno okno, pomijam.\")\n",
        "        continue\n",
        "\n",
        "    X_list.append(windows)\n",
        "    y_list.append(np.full((windows.shape[0],), label, dtype=\"int32\"))\n",
        "\n",
        "X = np.concatenate(X_list, axis=0)  # (N, SAMPLE_SIZE, 24)\n",
        "y = np.concatenate(y_list, axis=0)  # (N,)\n",
        "\n",
        "print(\"X shape:\", X.shape)\n",
        "print(\"y shape:\", y.shape, \"unikalne etykiety:\", np.unique(y))\n",
        "\n",
        "input_shape = (SAMPLE_SIZE, N_FEATURES)\n",
        "print(\"input_shape modelu:\", input_shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HqbKEzVAenJ"
      },
      "source": [
        "# 4. Frequency Domain Analysis (FFT)\n",
        "\n",
        "Mechanical faults in rotating components (such as propellers) generate distinct vibration signatures that are often most discernible in the frequency spectrum.\n",
        "\n",
        "In this experiment, we utilize data pre-processed via **Fast Fourier Transform (FFT)** using a Hann window to mitigate spectral leakage, which is already done in repo. The input features are vectors of spectral coefficients for each sensor axis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcZROUVfA4QE"
      },
      "source": [
        "## 4.1 Spectral Data Structure\n",
        "The FFT files contain metadata within their filenames (window length, window type, frequency range). The following code parses this information and loads the corresponding spectral coefficients."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3n9BPu7CQgA"
      },
      "outputs": [],
      "source": [
        "# Configuration for FFT Data\n",
        "FFT_ROOT      = \"FFT_data\"\n",
        "FFT_CONFIG    = \"256_Hann_40_104\"    # Specific window/range configuration\n",
        "SENSOR_MODE   = \"both\"               # \"accel\", \"gyro\", or \"both\"\n",
        "SAMPLING_RATE = 500.0                # Sampling rate for Bebop 2 inertial sensors\n",
        "\n",
        "# liczba osi na jeden typ czujnika\n",
        "N_AXES_SINGLE = 12\n",
        "N_AXES = 12 if SENSOR_MODE in (\"accel\", \"gyro\") else 24\n",
        "\n",
        "fft_dir = os.path.join(FFT_ROOT, FFT_CONFIG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DACo4DG-DI95"
      },
      "outputs": [],
      "source": [
        "def print_fft_info(fft_dir, sampling_rate=500.0):\n",
        "    \"\"\"\n",
        "    Extracts FFT parameters encoded in the data folder name and converts them\n",
        "    into physical frequency values.\n",
        "\n",
        "    The function assumes the folder name follows the format:\n",
        "    'WindowLength_WindowType_StartBin_StopBin' (e.g., '128_Hann_20_52').\n",
        "    \"\"\"\n",
        "\n",
        "    folder_name = os.path.basename(os.path.normpath(fft_dir))\n",
        "    parts = folder_name.split(\"_\")          # np. [\"128\",\"Hann\",\"20\",\"52\"]\n",
        "    measuringWindowLength = int(parts[0])   # 128\n",
        "    rangeStart = int(parts[-2])             # 20\n",
        "    rangeStop  = int(parts[-1])             # 52\n",
        "\n",
        "    freq_res = sampling_rate / measuringWindowLength\n",
        "    f_start  = (rangeStart - 1) * freq_res\n",
        "    f_stop   = rangeStop * freq_res\n",
        "\n",
        "    print(f\"Folder FFT: {folder_name}\")\n",
        "    print(f\"measuringWindowLength = {measuringWindowLength}\")\n",
        "    print(f\"Zakres binów: {rangeStart}–{rangeStop}\")\n",
        "    print(f\"Rozdzielczość częstotliwości: {freq_res:.3f} Hz\")\n",
        "    print(f\"Zakres częstotliwości: {f_start:.1f} Hz – {f_stop:.1f} Hz\")\n",
        "\n",
        "    return measuringWindowLength, rangeStart, rangeStop, freq_res, f_start, f_stop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yfm-tRZ0BT6D"
      },
      "outputs": [],
      "source": [
        "measuringWindowLength, rangeStart, rangeStop, freq_res, f_start, f_stop = print_fft_info(fft_dir, SAMPLING_RATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlzC6qT4cUds"
      },
      "source": [
        "### File Discovery & Categorization\n",
        "This block scans the directory for all CSV files and organizes them into dictionaries based on the sensor type (**ACCEL** vs. **GYRO**). It parses the filename to extract the specific fault scenario code (e.g., `0000`, `1022`), using it as a key for quick lookup during the data loading phase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jeyN1TjjCrJh"
      },
      "outputs": [],
      "source": [
        "# Scan directory and map file paths t# Scan directory and map file paths to scenarios based on sensor type (ACCEL/GYRO)o scenarios based on sensor type (ACCEL/GYRO)\n",
        "\n",
        "all_files = glob.glob(os.path.join(fft_dir, \"*.csv\"))\n",
        "\n",
        "accel_files = {}  # scenario -> ścieżka\n",
        "gyro_files  = {}\n",
        "\n",
        "for path in all_files:\n",
        "    fname = os.path.basename(path)\n",
        "    parts = fname.split(\"_\")\n",
        "    # przykład: Bebop2_16g_FFT_ACCEL_128_Hann_20_52_0000.csv\n",
        "    # indeksy:   0      1   2   3     4    5    6   7   8\n",
        "    sensor_type = parts[3]              # \"ACCEL\" albo \"GYRO\"\n",
        "    scenario    = os.path.splitext(parts[-1])[0]  # \"0000\" itd.\n",
        "\n",
        "    if sensor_type == \"ACCEL\":\n",
        "        accel_files[scenario] = path\n",
        "    elif sensor_type == \"GYRO\":\n",
        "        gyro_files[scenario] = path\n",
        "\n",
        "print(\"Znaleziono ACCEL dla scenariuszy:\", sorted(accel_files.keys()))\n",
        "print(\"Znaleziono GYRO  dla scenariuszy:\", sorted(gyro_files.keys()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjKbIR4PYZDm"
      },
      "source": [
        "### Data Loading & Sensor Fusion\n",
        "In this step, we aggregate the spectral data based on the selected `SENSOR_MODE`.\n",
        "* **Accel/Gyro:** Loads only the specified sensor data.\n",
        "* **Both:** Loads both accelerometer and gyroscope files for the same scenario, verifying consistency, and concatenates them along the feature axis to create a unified feature vector (e.g., 24 input channels)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_NuRvTiCaAm"
      },
      "outputs": [],
      "source": [
        "def load_fft_file(path, n_axes=N_AXES_SINGLE):\n",
        "    \"\"\"\n",
        "    Loads spectral data from a CSV file and reshapes it into a 3D tensor.\n",
        "\n",
        "    The function reads a flat CSV (assuming no header), calculates the number\n",
        "    of frequency bins based on the total columns and specified axes, and\n",
        "    restructures the data.\n",
        "    \"\"\"\n",
        "\n",
        "    # jeśli okaże się, że plik ma nagłówek – zmień na header=0\n",
        "    df = pd.read_csv(path, header=None)\n",
        "    data = df.values.astype(\"float32\")   # (n_okien, n_features)\n",
        "    n_features = data.shape[1]\n",
        "\n",
        "    if n_features % n_axes != 0:\n",
        "        raise ValueError(f\"{os.path.basename(path)}: {n_features} kolumn \"\n",
        "                         f\"nie dzieli się przez {n_axes} osi.\")\n",
        "\n",
        "    n_freq_bins = n_features // n_axes\n",
        "    data_3d = data.reshape(-1, n_freq_bins, n_axes)  # (n_okien, n_freq_bins, n_axes)\n",
        "    return data_3d, n_freq_bins"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqwyc0TwCjGF"
      },
      "outputs": [],
      "source": [
        "X_list = []\n",
        "y_list = []\n",
        "n_freq_bins_global = None\n",
        "\n",
        "for scenario, label in scenario_to_class.items():\n",
        "    cur_X = None\n",
        "\n",
        "    if SENSOR_MODE == \"accel\":\n",
        "        path = accel_files.get(scenario)\n",
        "        if path is None:\n",
        "            print(f\"[ACCEL] brak pliku dla scenariusza {scenario}, pomijam.\")\n",
        "            continue\n",
        "        accel_data, n_freq_bins = load_fft_file(path)\n",
        "        cur_X = accel_data   # (n_okien, n_freq_bins, 12)\n",
        "\n",
        "    elif SENSOR_MODE == \"gyro\":\n",
        "        path = gyro_files.get(scenario)\n",
        "        if path is None:\n",
        "            print(f\"[GYRO] brak pliku dla scenariusza {scenario}, pomijam.\")\n",
        "            continue\n",
        "        gyro_data, n_freq_bins = load_fft_file(path)\n",
        "        cur_X = gyro_data    # (n_okien, n_freq_bins, 12)\n",
        "\n",
        "    elif SENSOR_MODE == \"both\":\n",
        "        path_a = accel_files.get(scenario)\n",
        "        path_g = gyro_files.get(scenario)\n",
        "        if path_a is None or path_g is None:\n",
        "            print(f\"[BOTH] brak ACCEL lub GYRO dla {scenario}, pomijam.\")\n",
        "            continue\n",
        "\n",
        "        accel_data, n_freq_bins_a = load_fft_file(path_a, n_axes=N_AXES_SINGLE)\n",
        "        gyro_data,  n_freq_bins_g = load_fft_file(path_g, n_axes=N_AXES_SINGLE)\n",
        "\n",
        "        if accel_data.shape[0] != gyro_data.shape[0] or n_freq_bins_a != n_freq_bins_g:\n",
        "            raise ValueError(f\"Niezgodne rozmiary ACCEL/GYRO dla scenariusza {scenario}\")\n",
        "\n",
        "        # sklejanie po osi „kanałów”: 12 (ACCEL) + 12 (GYRO) = 24\n",
        "        cur_X = np.concatenate([accel_data, gyro_data], axis=-1)  # (..., n_freq_bins, 24)\n",
        "        n_freq_bins = n_freq_bins_a\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"SENSOR_MODE musi być 'accel', 'gyro' albo 'both'\")\n",
        "\n",
        "    # ustaw / sprawdź globalną liczbę binów\n",
        "    if n_freq_bins_global is None:\n",
        "        n_freq_bins_global = n_freq_bins\n",
        "    elif n_freq_bins_global != n_freq_bins:\n",
        "        raise ValueError(\"Różne n_freq_bins między plikami, coś jest nie tak.\")\n",
        "\n",
        "    X_list.append(cur_X)\n",
        "    y_list.append(np.full((cur_X.shape[0],), label, dtype=\"int32\"))\n",
        "\n",
        "# Sklejenie wszystkiego\n",
        "X = np.concatenate(X_list, axis=0)   # (N, n_freq_bins, N_AXES)\n",
        "y = np.concatenate(y_list, axis=0)   # (N,)\n",
        "\n",
        "print(\"X shape:\", X.shape)\n",
        "print(\"y shape:\", y.shape, \"unikalne etykiety:\", np.unique(y))\n",
        "\n",
        "n_freq_bins = n_freq_bins_global\n",
        "input_shape = (n_freq_bins, N_AXES)\n",
        "print(\"input_shape modelu:\", input_shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4l-eqQtTDsyN"
      },
      "source": [
        "# 5. Models Creation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Controller of the W&B logger"
      ],
      "metadata": {
        "id": "Z5y92gBh0JCK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Wybierz jeden tryb:\n",
        "# \"online\"   -> Wysyła od razu do chmury\n",
        "# \"offline\"  -> Zapisuje na dysku (wymaga wandb sync)\n",
        "# \"disabled\" -> Nic nie robi (debugowanie)\n",
        "LOG_MODE = \"online\"\n",
        "\n",
        "SHOULD_LOG = (LOG_MODE != \"disabled\")"
      ],
      "metadata": {
        "id": "q50622200IC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qgCY9_PGXKD"
      },
      "source": [
        "## 5.1 ANN - MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VL33vW1bD-HV"
      },
      "outputs": [],
      "source": [
        "# Initialize W&B Experiment, CONFIG\n",
        "wandb.init(\n",
        "    #project=\"UAV-FDI-Bebop2\",\n",
        "    project=\"UAV-FDI\",\n",
        "    name=\"Time_MLP_v3\",\n",
        "    mode=LOG_MODE,\n",
        "    config={\n",
        "        \"domain\": \"time\",\n",
        "        \"sensor_mode\": SENSOR_MODE,\n",
        "        \"input_shape\": input_shape,\n",
        "        \"output_shape\": NUM_CLASSES,\n",
        "        \"layer_1\": 128,\n",
        "        \"dropout_rate\": 0.2,\n",
        "        #\"l2_value\": 0.01,\n",
        "        \"activation\": \"relu\",\n",
        "        \"final_activation\": \"softmax\",\n",
        "        \"optimizer\": \"sgd\",\n",
        "        \"loss\": \"sparse_categorical_crossentropy\",\n",
        "        \"metric\": \"accuracy\",\n",
        "        \"learning_rate\": 0.001,\n",
        "        \"epochs\": 10,\n",
        "        \"batch_size\": 512\n",
        "    }\n",
        ")\n",
        "\n",
        "\n",
        "config = wandb.config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8Fri-lbJyNa"
      },
      "outputs": [],
      "source": [
        "model = models.Sequential([\n",
        "layers.Input(shape=input_shape),\n",
        "layers.Flatten(),\n",
        "layers.Dense(config.layer_1, activation=config.activation),\n",
        "layers.Dropout(config.dropout_rate),\n",
        "layers.Dense(NUM_CLASSES, activation=config.final_activation)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0KsHw0xGvPS"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=config.optimizer,\n",
        "    loss=config.loss,\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating model structure chart\n",
        "plot_model(model, to_file='model_arch.png', show_shapes=True, show_layer_names=True)\n",
        "\n",
        "wandb.log({\"charts/model_architecture\": wandb.Image('model_arch.png')})\n",
        "\n",
        "if LOG_MODE == \"online\":\n",
        "    print(\"Schemat modelu został wysłany do chmury W&B.\")\n",
        "\n",
        "elif LOG_MODE == \"offline\":\n",
        "    print(\"Zapisano lokalnie.\")\n",
        "\n",
        "else: # disabled\n",
        "    print(\"Logowanie wyłączone - nic nie wysłano.\")"
      ],
      "metadata": {
        "id": "HXuELopo1v-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uXTYHDSG1wR"
      },
      "source": [
        "# 6. Data splitting - train/val/test\n",
        "To ensure efficient training, we convert the NumPy arrays into `tf.data.Dataset` objects. We apply:\n",
        "* **Shuffling:** To prevent the model from learning order-dependent patterns.\n",
        "* **Batching:** Grouping samples for gradient updates.\n",
        "* **Prefetching:** Using `AUTOTUNE` to prepare the next batch of data while the GPU is training on the current one, significantly reducing latency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JyyRLJJcG1ET"
      },
      "outputs": [],
      "source": [
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
        ")\n",
        "\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)) \\\n",
        "    .shuffle(buffer_size=len(X_train)) \\\n",
        "    .batch(config.batch_size) \\\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val)) \\\n",
        "    .batch(config.batch_size) \\\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)) \\\n",
        "    .batch(config.batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbfmWiycEi04"
      },
      "source": [
        "# 6. Training, Evaluation, and Experiment Logging\n",
        "\n",
        "This comprehensive block executes the core machine learning pipeline:\n",
        "\n",
        "1.  **Training:** The model is trained using `model.fit()` with callbacks to:\n",
        "    * Log live training metrics to W&B (`WandbMetricsLogger`).\n",
        "    * Save the best model weights based on validation accuracy (`ModelCheckpoint`).\n",
        "2.  **Inference & Metrics:** After training, the model generates predictions on the held-out test set. We calculate key performance indicators: **Accuracy**, **Precision**, **Recall**, and **F1-Score** (macro-averaged).\n",
        "3.  **Visualization:** Standard diagnostic plots are generated using Matplotlib:\n",
        "    * **Learning Curves:** Loss and Accuracy over epochs.\n",
        "    * **Confusion Matrix:** To visualize misclassifications between classes.\n",
        "    * **ROC Curves:** To analyze the trade-off between True Positive Rate and False Positive Rate for each class.\n",
        "4.  **Final Logging:** All computed scalars, generated plots, and the saved model artifact are uploaded to the **Weights & Biases** dashboard to finalize the experiment run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uP-2eRM5Emgw"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# 3. TRENOWANIE\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"Rozpoczynam trenowanie...\")\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=config.epochs,\n",
        "    callbacks=[\n",
        "        # Loguje przebieg treningu (krzywe live)\n",
        "        WandbMetricsLogger(log_freq=5),\n",
        "        # Zapisuje najlepszy model (wymagane .keras dla Keras 3)\n",
        "        ModelCheckpoint(\n",
        "            filepath=\"checkpoints/model_best.keras\",\n",
        "            save_best_only=True,\n",
        "            monitor=\"val_accuracy\",\n",
        "            mode=\"max\",\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. EWALUACJA I OBLICZANIE METRYK\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\nGenerowanie predykcji na zbiorze testowym...\")\n",
        "y_true_list = []\n",
        "y_proba_list = []\n",
        "\n",
        "for x_batch, y_batch in test_ds:\n",
        "    y_true_list.append(y_batch.numpy())\n",
        "    y_proba_list.append(model.predict(x_batch, verbose=0))\n",
        "\n",
        "y_true = np.concatenate(y_true_list, axis=0)   # (N,)\n",
        "y_proba = np.concatenate(y_proba_list, axis=0) # (N, NUM_CLASSES)\n",
        "y_pred = np.argmax(y_proba, axis=1)            # (N,)\n",
        "\n",
        "# Upewniamy się, że y_true to inty\n",
        "if y_true.ndim > 1:\n",
        "    y_true_int = np.argmax(y_true, axis=1)\n",
        "else:\n",
        "    y_true_int = y_true.astype(int)\n",
        "\n",
        "# --- Metryki Liczbowe ---\n",
        "test_loss, test_acc = model.evaluate(test_ds, verbose=0)\n",
        "prec = precision_score(y_true_int, y_pred, average=\"macro\")\n",
        "rec = recall_score(y_true_int, y_pred, average=\"macro\")\n",
        "f1 = f1_score(y_true_int, y_pred, average=\"macro\")\n",
        "\n",
        "print(f\"Loss: {test_loss:.4f}, Acc: {test_acc:.4f}, F1: {f1:.4f}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 5. TWORZENIE WYKRESÓW (MATPLOTLIB)\n",
        "# ==============================================================================\n",
        "\n",
        "# A. Loss Plot\n",
        "fig_loss = plt.figure(figsize=(8, 5))\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "plt.title('Training Loss')\n",
        "plt.legend(); plt.grid(True)\n",
        "\n",
        "# B. Accuracy Plot\n",
        "fig_acc = plt.figure(figsize=(8, 5))\n",
        "plt.plot(history.history['accuracy'], label='Train Acc')\n",
        "plt.plot(history.history['val_accuracy'], label='Val Acc')\n",
        "plt.title('Training Accuracy')\n",
        "plt.legend(); plt.grid(True)\n",
        "\n",
        "# C. Confusion Matrix\n",
        "cm = confusion_matrix(y_true_int, y_pred)\n",
        "fig_cm, ax = plt.subplots(figsize=(8, 8))\n",
        "ConfusionMatrixDisplay(cm).plot(ax=ax, cmap='Blues', values_format='d')\n",
        "plt.title('Confusion Matrix')\n",
        "\n",
        "# D. ROC Curve\n",
        "y_true_bin = label_binarize(y_true_int, classes=np.arange(NUM_CLASSES))\n",
        "fpr, tpr, roc_auc = {}, {}, {}\n",
        "for i in range(NUM_CLASSES):\n",
        "    fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_proba[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "fig_roc = plt.figure(figsize=(10, 8))\n",
        "for i in range(NUM_CLASSES):\n",
        "    plt.plot(fpr[i], tpr[i], label=f'Cls {i} (AUC={roc_auc[i]:.2f})')\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.title('ROC Curve')\n",
        "# Legenda poza wykresem, żeby nie zasłaniała przy wielu klasach\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.tight_layout()\n",
        "\n",
        "# 2. RĘCZNE WYSŁANIE MODELU NA KOŃCU\n",
        "# Robimy to tylko raz, tuż przed wandb.finish()\n",
        "wandb.save(\"checkpoints/model_best.keras\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 6. LOGOWANIE FINALNE (Podział na Summary i Media)\n",
        "# ==============================================================================\n",
        "\n",
        "# 1. LICZBY -> wandb.summary (Tylko do tabeli, BEZ kropek na wykresach)\n",
        "wandb.summary[\"test_loss\"] = test_loss\n",
        "wandb.summary[\"test_accuracy\"] = test_acc\n",
        "wandb.summary[\"test_precision\"] = prec\n",
        "wandb.summary[\"test_recall\"] = rec\n",
        "wandb.summary[\"test_f1_score\"] = f1\n",
        "\n",
        "# 2. OBRAZKI -> wandb.log (Do sekcji Media / Custom Charts)\n",
        "wandb.log({\n",
        "    \"charts/loss_history\": wandb.Image(fig_loss),\n",
        "    \"charts/acc_history\": wandb.Image(fig_acc),\n",
        "    \"charts/confusion_matrix\": wandb.Image(fig_cm),\n",
        "    \"charts/roc_curve\": wandb.Image(fig_roc)\n",
        "})\n",
        "\n",
        "# Sprzątanie pamięci RAM (zamykamy wykresy matplotlib)\n",
        "plt.close('all')\n",
        "\n",
        "if LOG_MODE == \"online\":\n",
        "    print(\"Proces zakończony. Wyniki wysłane do chmury W&B.\")\n",
        "\n",
        "elif LOG_MODE == \"offline\":\n",
        "    print(\"Proces zakończony. Wyniki zapisano lokalnie.\")\n",
        "\n",
        "else: # disabled\n",
        "    print(\"Proces zakończony.\")\n",
        "    print(\"Logowanie wyłączone - nic nie wysłano.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCBcW4HdL7_Z"
      },
      "source": [
        "## Local Result Visualization\n",
        "This section serves as an immediate, on-screen verification of the training results. It prints the final classification metrics (Accuracy, Loss, Precision, Recall, F1) directly to the console and renders the learning curves and confusion matrix inline using Matplotlib."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wR7n4hRLLH0"
      },
      "outputs": [],
      "source": [
        "# === 1. WYPISANIE WYNIKÓW LICZBOWYCH ===\n",
        "print(\"=\"*40)\n",
        "print(f\"RAPORT KOŃCOWY MODELU\")\n",
        "print(\"=\"*40)\n",
        "print(f\"Test Accuracy:  {test_acc:.2%}\")      # Format procentowy\n",
        "print(f\"Test Loss:      {test_loss:.4f}\")\n",
        "print(\"-\"*20)\n",
        "print(f\"Precision:      {prec:.4f}\")\n",
        "print(f\"Recall:         {rec:.4f}\")\n",
        "print(f\"F1 Score:       {f1:.4f}\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "# === 2. RYSOWANIE WYKRESÓW HISTORII (LOSS & ACCURACY) ===\n",
        "# Tworzymy jeden duży obrazek z dwoma wykresami obok siebie\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Wykres Loss\n",
        "ax1.plot(history.history['loss'], label='Train Loss')\n",
        "ax1.plot(history.history['val_loss'], label='Val Loss')\n",
        "ax1.set_title('Loss (Błąd)')\n",
        "ax1.set_xlabel('Epoka')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.legend()\n",
        "ax1.grid(True)\n",
        "\n",
        "# Wykres Accuracy\n",
        "ax2.plot(history.history['accuracy'], label='Train Acc')\n",
        "ax2.plot(history.history['val_accuracy'], label='Val Acc')\n",
        "ax2.set_title('Accuracy (Dokładność)')\n",
        "ax2.set_xlabel('Epoka')\n",
        "ax2.set_ylabel('Acc')\n",
        "ax2.legend()\n",
        "ax2.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# === 3. RYSOWANIE MACIERZY POMYŁEK (CONFUSION MATRIX) ===\n",
        "# Korzystamy z wcześniej obliczonych y_true_int i y_pred\n",
        "fig, ax = plt.subplots(figsize=(12, 12))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm) # cm jest w pamięci z poprzedniej komórki\n",
        "disp.plot(ax=ax, cmap='Blues', values_format='d')\n",
        "plt.title('Macierz Pomyłek (Test Set)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQYSHGyBY6EW"
      },
      "source": [
        "# 7. Specialized Quality metrics\n",
        "Standard accuracy can sometimes be misleading in multi-class problems. Here, we calculate additional metrics proposed for fault diagnosis:\n",
        "* **$A$ (Average Accuracy):** Mean accuracy across all classes.\n",
        "* **$Q_{AC}$ (Accurate Classification Quality):** A metric that rewards high accuracy while penalizing performance on a large number of classes (based on information theory).\n",
        "* **Inference Time ($T$):** Measuring the computational cost of a single prediction to assess real-time feasibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3DrDSJbHkKl"
      },
      "outputs": [],
      "source": [
        "# === 1. Zbieramy y_true i y_pred na zbiorze testowym ===\n",
        "y_true_list = []\n",
        "y_pred_list = []\n",
        "\n",
        "for x_batch, y_batch in test_ds:\n",
        "    # y_batch: numery klas 0..NUM_CLASSES-1 (jeśli one-hot, damy radę niżej)\n",
        "    y_true_list.append(y_batch.numpy())\n",
        "    y_pred_list.append(\n",
        "        np.argmax(model.predict(x_batch, verbose=0), axis=1)\n",
        "    )\n",
        "\n",
        "y_true = np.concatenate(y_true_list, axis=0)\n",
        "y_pred = np.concatenate(y_pred_list, axis=0)\n",
        "\n",
        "# jeśli y_true jest one-hot → zrzucamy do indeksów klas\n",
        "if y_true.ndim > 1:\n",
        "    y_true_int = np.argmax(y_true, axis=1)\n",
        "else:\n",
        "    y_true_int = y_true.astype(int)\n",
        "\n",
        "# === 2. Confusion matrix ===\n",
        "cm = confusion_matrix(y_true_int, y_pred)\n",
        "#print(\"Confusion matrix:\\n\", cm)\n",
        "\n",
        "# === 3. Funkcja do obliczania A, Q_AC, Q_ACT wg wzorów z artykułu ===\n",
        "def compute_quality_indices(cm, alpha=3.0, T_ms=None, T0_ms=1.0):\n",
        "    \"\"\"\n",
        "    cm    : macierz pomyłek (C x C)\n",
        "    alpha : indeks potęgowy α (>1)\n",
        "    T_ms  : czas klasyfikacji w ms (dla Q_ACT), jeśli None – Q_ACT nie jest liczony\n",
        "    T0_ms : okres odniesienia (1 ms dla 1 kHz)\n",
        "    \"\"\"\n",
        "    C = cm.shape[0]           # liczba klas\n",
        "    N = cm.sum()              # liczba wszystkich próbek\n",
        "\n",
        "    TP = np.diag(cm)\n",
        "    FN = cm.sum(axis=1) - TP\n",
        "    FP = cm.sum(axis=0) - TP\n",
        "    TN = N - TP - FN - FP\n",
        "\n",
        "    # Eq. (10): averaged accuracy A\n",
        "    per_class_acc = (TP + TN) / (TP + FP + TN + FN)\n",
        "    A = per_class_acc.mean()\n",
        "\n",
        "    # Eq. (9): Q_AC = A^α * sqrt(ln C)\n",
        "    Q_AC = (A ** alpha) * np.sqrt(np.log(C))\n",
        "\n",
        "    # Eq. (11): Q_ACT = A^α * sqrt( ln C / ln(1 + τ^2) ), τ = T/T0\n",
        "    Q_ACT = None\n",
        "    tau = None\n",
        "    if T_ms is not None:\n",
        "        tau = T_ms / T0_ms\n",
        "        Q_ACT = (A ** alpha) * np.sqrt(np.log(C) / np.log(1.0 + tau**2))\n",
        "\n",
        "    return {\n",
        "        \"A\": A,\n",
        "        \"per_class_acc\": per_class_acc,\n",
        "        \"Q_AC\": Q_AC,\n",
        "        \"Q_ACT\": Q_ACT,\n",
        "        \"tau\": tau,\n",
        "    }\n",
        "\n",
        "# === 4. Pomiar czasu JEDNEJ klasyfikacji (T) ===\n",
        "# bierzemy jedną próbkę ze zbioru testowego\n",
        "for xb, yb in test_ds.take(1):\n",
        "    sample = xb[0:1]   # kształt (1, n_freq_bins, n_axes)\n",
        "    break\n",
        "\n",
        "# mały \"warm-up\", żeby pominąć jednorazowe opóźnienia\n",
        "_ = model.predict(sample, verbose=0)\n",
        "\n",
        "N_REPEAT = 100  # ile razy powtarzamy, żeby uśrednić\n",
        "start = time.time()\n",
        "for _ in range(N_REPEAT):\n",
        "    _ = model.predict(sample, verbose=0)\n",
        "end = time.time()\n",
        "\n",
        "ANN_T_ms = (end - start) * 1000.0 / N_REPEAT   # średni czas jednej klasyfikacji w ms\n",
        "\n",
        "# === 5. Liczymy wskaźniki ===\n",
        "metrics_q = compute_quality_indices(cm, alpha=3.0)\n",
        "\n",
        "print(\"\\n=== Specjalistyczne metryki (wieloklasowe) ===\\n\")\n",
        "print(f\"A (średnia accuracy po klasach): {metrics_q['A']:.4f}\")\n",
        "print(f\"Q_AC (Accurate Classification Quality): {metrics_q['Q_AC']:.4f}\")\n",
        "print(f\"Czas jednej klasyfikacji T: {ANN_T_ms:.3f} ms\")\n",
        "#print(f\"Q_ACT (Accurate Classification-Time Quality): {metrics_q['Q_ACT']:.4f}\")\n",
        "print(\"Accuracy per klasa:\", metrics_q[\"per_class_acc\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78TiWS4aRmjY"
      },
      "outputs": [],
      "source": [
        "wandb.summary[\"Q_AC\"] = metrics_q['Q_AC']\n",
        "\n",
        "if LOG_MODE == \"online\":\n",
        "    print(\"Specjalistyczna metryka Accurate Classification Quality wysłana do chmury W&B.\")\n",
        "\n",
        "elif LOG_MODE == \"offline\":\n",
        "    print(\"Specjalistyczna metryka Accurate Classification Quality zapisana lokalnie.\")\n",
        "\n",
        "else: # disabled\n",
        "    print(\"Logowanie wyłączone - nic nie wysłano.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "US6F08OdRm3U"
      },
      "source": [
        "# W&B Experiment finish"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9uJn_MzRhTM"
      },
      "outputs": [],
      "source": [
        "# === KONIEC EKSPERYMENTU W&B ===\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### W&B SYNC\n",
        "\n",
        "Only necessary when W&B 'offline' mode is set."
      ],
      "metadata": {
        "id": "sRYYppsYBdOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#wandb sync wandb/latest-run"
      ],
      "metadata": {
        "id": "mVzi0x4QB867"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlD8ossoH7jX"
      },
      "source": [
        "# 8. Model Export\n",
        "\n",
        "To facilitate deployment on the drone's embedded platform or for future inference tasks, we serialize the trained model along with a metadata file. The `metadata.json` file contains critical configuration details regarding sensor setup and class mapping, which are essential for correctly interpreting model predictions in a production environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-EW4rF3sH8Am"
      },
      "outputs": [],
      "source": [
        "# katalog na zapisane modele\n",
        "SAVE_DIR = \"saved_models\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "MODEL_PATH    = os.path.join(SAVE_DIR, \"uav_fft_classifier.keras\")  # format Keras\n",
        "METADATA_PATH = os.path.join(SAVE_DIR, \"metadata.json\")\n",
        "\n",
        "# Tworzymy słownik odwrotny: ID -> Nazwa scenariusza (do odczytu predykcji)\n",
        "class_to_scenario = {v: k for k, v in scenario_to_class.items()}\n",
        "\n",
        "# 1) zapis samego modelu\n",
        "model.save(MODEL_PATH)\n",
        "print(\"Zapisano model do:\", MODEL_PATH)\n",
        "\n",
        "# 2) zapis podstawowych metadanych (przydadzą się przy wczytywaniu)\n",
        "metadata = {\n",
        "    \"data_domain\": config.domain,\n",
        "    \"num_classes\": int(NUM_CLASSES),\n",
        "    \"classes_mapping\": class_to_scenario,\n",
        "    \"input_shape\": input_shape,\n",
        "    \"sensor_mode\": SENSOR_MODE,\n",
        "    \"data_files_pattern\": norm_data_files_pattern,   # norm_data_files_pattern lub FFT_CONFIG\n",
        "    \"final_test_accuracy\": float(test_acc),\n",
        "    \"final_f1_score\": float(f1)\n",
        "}\n",
        "\n",
        "with open(METADATA_PATH, \"w\") as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "\n",
        "print(\"Zapisano metadane do:\", METADATA_PATH)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Single Inference & Real-world Simulation\n",
        "\n",
        "This cell simulates a production environment scenario to verify the model's performance on individual samples. It performs the following steps:\n",
        "1.  **Model Loading:** Reloads the saved model (`.keras`) from the disk to ensure the artifact is valid and ready for deployment.\n",
        "2.  **Random Sampling:** Selects a single, random sample from the test dataset to simulate an incoming data stream.\n",
        "3.  **Inference:** Runs the classification and calculates the confidence score.\n",
        "4.  **Decoding:** Maps the numerical class ID back to a human-readable fault scenario (e.g., identifying specifically which rotor is damaged) using the `scenario_to_class` mapping."
      ],
      "metadata": {
        "id": "u7uIl0GFZvpT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. CONFIGURATION & MAPPING (DECODER RING)\n",
        "# ==============================================================================\n",
        "\n",
        "# Fault type definitions based on your description\n",
        "FAULT_MEANING = {\n",
        "    '0': \"✅ no fault\",\n",
        "    '1': \"⚠️ chipped edge\",\n",
        "    '2': \"❌ bent tip\"\n",
        "}\n",
        "\n",
        "# Create inverse dictionary: Class ID -> Scenario Code (e.g., 10 -> \"1020\")\n",
        "current_map = scenario_to_class_20\n",
        "class_to_scenario = {v: k for k, v in current_map.items()}\n",
        "\n",
        "def decode_scenario_string(scenario_code):\n",
        "    \"\"\"\n",
        "    Decodes the string (e.g., '1020') into human-readable rotor status.\n",
        "    Assumption: The string digits correspond to Rotors [A, B, C, D].\n",
        "    \"\"\"\n",
        "    description = r\"\"\"\n",
        "    Rotors Layout:\n",
        "       C   A\n",
        "        \\ /\n",
        "        / \\\n",
        "       D   B\n",
        "    \"\"\"\n",
        "    # Mapping indices 0,1,2,3 to the Rotors A,B,C,D and their positions\n",
        "    rotors_mapping = [\n",
        "        \"Rotor A (Front-Right)\", # Index 0\n",
        "        \"Rotor B (Rear-Right) \", # Index 1\n",
        "        \"Rotor C (Front-Left) \", # Index 2\n",
        "        \"Rotor D (Rear-Left)  \"  # Index 3\n",
        "    ]\n",
        "\n",
        "    print(f\"\\n Detailed UAV State Analysis (Code: {scenario_code}):\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    # Iterate through the characters of the string\n",
        "    for i, char in enumerate(scenario_code):\n",
        "        if i < len(rotors_mapping):\n",
        "            rotor_name = rotors_mapping[i]\n",
        "            status = FAULT_MEANING.get(char, \"Unknown State\")\n",
        "            print(f\"  🚁 {rotor_name:<25}: {status}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. LOAD MODEL & GET RANDOM SAMPLE\n",
        "# ==============================================================================\n",
        "\n",
        "# Path to your saved model (ensure this matches your SAVE_DIR)\n",
        "MODEL_PATH = \"saved_models/uav_fft_classifier.keras\"\n",
        "\n",
        "print(f\"Loading model from: {MODEL_PATH} ...\")\n",
        "try:\n",
        "    loaded_model = load_model(MODEL_PATH)\n",
        "    print(\"Model loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")\n",
        "    # Fallback to the current 'model' variable if file not found (for testing in NB)\n",
        "    loaded_model = model\n",
        "\n",
        "# Get one batch from the test dataset\n",
        "print(\"Sampling a random input from test_ds...\")\n",
        "for x_batch, y_batch in test_ds.take(1):\n",
        "    # Pick a random index within this batch\n",
        "    random_idx = np.random.randint(0, len(x_batch))\n",
        "\n",
        "    # Extract single sample\n",
        "    sample_x = x_batch[random_idx] # Shape (65, 24)\n",
        "    sample_y = y_batch[random_idx] # True label\n",
        "\n",
        "    # The model expects a batch dimension, so we expand: (1, 65, 24)\n",
        "    sample_x_expanded = np.expand_dims(sample_x, axis=0)\n",
        "\n",
        "    # ==========================================================================\n",
        "    # 3. PERFORM INFERENCE\n",
        "    # ==========================================================================\n",
        "\n",
        "    # Predict\n",
        "    predictions = loaded_model(sample_x_expanded, training=False).numpy()\n",
        "    pred_class_id = np.argmax(predictions)\n",
        "    confidence = np.max(predictions) * 100\n",
        "\n",
        "    # ==========================================================================\n",
        "    # 4. DECODE & REPORT\n",
        "    # ==========================================================================\n",
        "\n",
        "    # Handle true label format (one-hot vs integer)\n",
        "    if np.ndim(sample_y) > 0 and len(sample_y) > 1:\n",
        "        true_class_id = np.argmax(sample_y)\n",
        "    else:\n",
        "        true_class_id = int(sample_y)\n",
        "\n",
        "    # Convert ID -> Scenario Code (e.g., 12 -> \"0120\")\n",
        "    pred_scenario_code = class_to_scenario.get(pred_class_id, \"????\")\n",
        "    true_scenario_code = class_to_scenario.get(true_class_id, \"????\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"SINGLE INFERENCE RESULT\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Visual check\n",
        "    is_correct = (pred_class_id == true_class_id)\n",
        "    icon = \"✅ SUCCESS\" if is_correct else \"❌ FAILURE\"\n",
        "\n",
        "    print(f\"True Class:        {true_class_id} [{true_scenario_code}]\")\n",
        "    print(f\"Predicted Class:   {pred_class_id} [{pred_scenario_code}]\")\n",
        "    print(f\"Model Confidence:  {confidence:.2f}%\")\n",
        "    print(f\"Inference Status:  {icon}\")\n",
        "\n",
        "    # Decode the predicted scenario to show rotor status\n",
        "    decode_scenario_string(pred_scenario_code)\n",
        "\n",
        "    if not is_correct:\n",
        "      print(\"\\n\\nTrue class scenario description\")\n",
        "      decode_scenario_string(true_scenario_code)\n",
        "\n",
        "    break # Stop after one sample"
      ],
      "metadata": {
        "id": "Mv7ztKL5Z2nx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMlYW0+I9q/jX9ccsb5TRfq",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}