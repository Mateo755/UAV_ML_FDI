{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# UAV Propeller Fault Detection System (Parrot Bebop 2)\n",
        "\n",
        "This project focuses on the development and validation of fault detection and isolation (FDI) methods for the propulsion system of the **Parrot Bebop 2** unmanned aerial vehicle (UAV). The analysis utilizes inertial sensor data (accelerometer and gyroscope) collected during real-world flight experiments.\n",
        "\n",
        "### Research Problem\n",
        "The primary objective is to classify the technical state of the propellers based on vibration signals. We analyze various fault scenarios across four rotors (A, B, C, D), distinguishing between nominal states and specific defects such as chipped edges or bent blades.\n",
        "\n",
        "### Methodology\n",
        "This notebook compares two signal processing approaches:\n",
        "1.  **Time Domain Analysis**\n",
        "2.  **Frequency Domain Analysis**\n",
        "\n",
        "Experiment tracking and performance visualization are managed via **Weights & Biases (W&B)**."
      ],
      "metadata": {
        "id": "CCI_N8xy07lW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Environment Setup & Global Configuration\n",
        "Installation of necessary libraries (Weights & Biases for experiment tracking) and importing standard data science modules."
      ],
      "metadata": {
        "id": "3Do56U9O8568"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XHNOxhxQz5nx"
      },
      "outputs": [],
      "source": [
        "!pip install wandb -q\n",
        "import wandb\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report, precision_score, \\\n",
        "                            recall_score, f1_score, roc_curve, auc, ConfusionMatrixDisplay\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from tensorflow.keras import layers, models, regularizers\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from wandb.integration.keras import WandbMetricsLogger, WandbModelCheckpoint\n",
        "\n",
        "# Login to W&B\n",
        "#wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Exploratory Data Analysis\n",
        "Initial inspection of the accelerometer and gyroscope data from the UAV. We examine the column structure."
      ],
      "metadata": {
        "id": "LoUfiIkK1LyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Time domain"
      ],
      "metadata": {
        "id": "-5cYD7jd2J0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q /content/Normalized_data.zip"
      ],
      "metadata": {
        "id": "EfFehWg_BIQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_time = pd.read_csv('/content/Bebop2_16g_1kdps_normalized_0000.csv')\n",
        "df_time"
      ],
      "metadata": {
        "id": "oUIOpXWM1LhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_time.info()"
      ],
      "metadata": {
        "id": "OXXPsSeP1cKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Frequency Domain"
      ],
      "metadata": {
        "id": "PzQLUTk82LtL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q /content/FFT_data.zip"
      ],
      "metadata": {
        "id": "c2ARDYyABHWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_freq = pd.read_csv('/content/FFT_data/128_Hann_20_52/Bebop2_16g_FFT_ACCEL_128_Hann_20_52_0000.csv', header=None)\n",
        "df_freq"
      ],
      "metadata": {
        "id": "mWm9whI32SFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_freq.info()"
      ],
      "metadata": {
        "id": "_cnT-vQb27cS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Fault Scenario Mapping\n",
        "\n",
        "The Bebop 2 flight data is labeled using a 4-digit code (e.g., `1022`), defining the state of each propeller (A, B, C, D):\n",
        "* **0**: Nominal (Functional propeller).\n",
        "* **1**: Fault Type I (e.g., chipped edge).\n",
        "* **2**: Fault Type II (e.g., bent tip/severe damage).\n",
        "\n",
        "Below, we define the mapping of these physical scenarios to model class labels. Depending on the diagnostic granularity required, the problem can be framed as a **5-class problem** (aggregated by the number of faults) or a **20-class problem** (precise fault configuration)."
      ],
      "metadata": {
        "id": "KpQQlUm49FYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select Experiment Mode\n",
        "CLASS_MODE = \"20class\"   # Options: \"5class\" (aggregated) or \"20class\" (precise diagnosis)"
      ],
      "metadata": {
        "id": "TPL1aCh09auz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Precise mapping for 20 unique scenarios (Parrot Bebop 2)\n",
        "scenario_to_class_20 = {\n",
        "    \"0000\": 0,  # Nominal state\n",
        "    \"1000\": 1, \"0100\": 2, \"0010\": 3, \"0001\": 4,     # Single faults (Type 1)\n",
        "    \"2000\": 5, \"0200\": 6, \"0020\": 7, \"0002\": 8,     # Single faults (Type 2)\n",
        "    \"1100\": 9, \"1020\": 10, \"1002\": 11, \"0120\": 12, \"0102\": 13, \"0022\": 14, # Dual faults\n",
        "    \"1120\": 15, \"1102\": 16, \"1022\": 17, \"0122\": 18, # Triple faults\n",
        "    \"1122\": 19, # All propellers faulty\n",
        "}\n",
        "\n",
        "# Simplified mapping (Number of faulty rotors)\n",
        "scenario_to_class_5 = {\n",
        "    \"0000\": 0,\n",
        "    \"1000\": 1, \"0100\": 1, \"0010\": 1, \"0001\": 1,\n",
        "    \"2000\": 1, \"0200\": 1, \"0020\": 1, \"0002\": 1,\n",
        "    \"1100\": 2, \"1020\": 2, \"1002\": 2, \"0120\": 2, \"0102\": 2, \"0022\": 2,\n",
        "    \"1120\": 3, \"1102\": 3, \"1022\": 3, \"0122\": 3,\n",
        "    \"1122\": 4,\n",
        "}\n",
        "\n",
        "if CLASS_MODE == \"5class\":\n",
        "    scenario_to_class = scenario_to_class_5\n",
        "elif CLASS_MODE == \"20class\":\n",
        "    scenario_to_class = scenario_to_class_20\n",
        "else:\n",
        "    raise ValueError(\"Invalid CLASS_MODE selected.\")\n",
        "\n",
        "NUM_CLASSES = len(set(scenario_to_class.values()))\n",
        "print(f\"Experiment Mode: {CLASS_MODE} | Total Classes: {NUM_CLASSES}\")"
      ],
      "metadata": {
        "id": "C9yIykYh9RcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Time Domain Analysis, data preparation\n",
        "\n",
        "In this section, we process normalized time-series signals from the accelerometers and gyroscopes. Since the data represents a continuous flight stream, we apply a **sliding window** technique to segment the signal into fixed-length samples (e.g., 256 measurement points).\n",
        "\n"
      ],
      "metadata": {
        "id": "qTBVnJlJ-oN6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Data Segmentation"
      ],
      "metadata": {
        "id": "aovzZqvy-8L9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to normalized time-domain data\n",
        "DATA_DIR_TIME = r\"/content/Normalized_data\"\n",
        "SAMPLE_SIZE = 256      # Window length\n",
        "N_FEATURES = 24        # Input channels (e.g., 3-axis accel + 3-axis gyro per sensor)"
      ],
      "metadata": {
        "id": "bRPd81ZC-nf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_windows_from_df(df: pd.DataFrame, sample_size: int):\n",
        "    \"\"\"\n",
        "    Segments time-series data into non-overlapping windows.\n",
        "    \"\"\"\n",
        "    data = df.values.astype(\"float32\")\n",
        "    n_total = len(data)\n",
        "    n_windows = n_total // sample_size\n",
        "    if n_windows == 0:\n",
        "        return np.empty((0, sample_size, data.shape[1]), dtype=\"float32\")\n",
        "    data = data[:n_windows * sample_size]\n",
        "    windows = data.reshape(n_windows, sample_size, data.shape[1])\n",
        "    return windows"
      ],
      "metadata": {
        "id": "ho9zzXAMDFeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data pass through window function\n",
        "\n",
        "X_list = []\n",
        "y_list = []\n",
        "\n",
        "pattern = os.path.join(DATA_DIR_TIME, \"Bebop2_16g_1kdps_normalized_*.csv\")\n",
        "\n",
        "for path in glob.glob(pattern):\n",
        "    fname = os.path.basename(path)\n",
        "    # ostatni fragment po \"_\" to kod scenariusza, np. \"0000\"\n",
        "    scenario = os.path.splitext(fname)[0].split(\"_\")[-1]\n",
        "\n",
        "    if scenario not in scenario_to_class:\n",
        "        print(f\"Pomijam {fname} – scenariusz {scenario} nie jest w mapowaniu.\")\n",
        "        continue\n",
        "\n",
        "    label = scenario_to_class[scenario]\n",
        "\n",
        "    df = pd.read_csv(path)\n",
        "\n",
        "    # jeżeli w pliku są inne kolumny niż 24 sensory, tu można wybrać tylko potrzebne:\n",
        "    # df = df[[\"A_aX\",\"A_aY\",...,\"D_gZ\"]]\n",
        "\n",
        "    windows = make_windows_from_df(df, SAMPLE_SIZE)\n",
        "    if windows.shape[0] == 0:\n",
        "        print(f\"Za mało danych w {fname} na choć jedno okno, pomijam.\")\n",
        "        continue\n",
        "\n",
        "    X_list.append(windows)\n",
        "    y_list.append(np.full((windows.shape[0],), label, dtype=\"int32\"))\n",
        "\n",
        "X = np.concatenate(X_list, axis=0)  # (N, SAMPLE_SIZE, 24)\n",
        "y = np.concatenate(y_list, axis=0)  # (N,)\n",
        "\n",
        "print(\"X shape:\", X.shape)\n",
        "print(\"y shape:\", y.shape, \"unikalne etykiety:\", np.unique(y))"
      ],
      "metadata": {
        "id": "gl0X-52e_V-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Frequency Domain Analysis (FFT)\n",
        "\n",
        "Mechanical faults in rotating components (such as propellers) generate distinct vibration signatures that are often most discernible in the frequency spectrum.\n",
        "\n",
        "In this experiment, we utilize data pre-processed via **Fast Fourier Transform (FFT)** using a Hann window to mitigate spectral leakage, which is already done in repo. The input features are vectors of spectral coefficients for each sensor axis."
      ],
      "metadata": {
        "id": "0HqbKEzVAenJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 Spectral Data Structure\n",
        "The FFT files contain metadata within their filenames (window length, window type, frequency range). The following code parses this information and loads the corresponding spectral coefficients."
      ],
      "metadata": {
        "id": "fcZROUVfA4QE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration for FFT Data\n",
        "FFT_ROOT      = \"FFT_data\"\n",
        "FFT_CONFIG    = \"256_Hann_40_104\"    # Specific window/range configuration\n",
        "SENSOR_MODE   = \"both\"               # \"accel\", \"gyro\", or \"both\"\n",
        "SAMPLING_RATE = 500.0                # Sampling rate for Bebop 2 inertial sensors\n",
        "\n",
        "# liczba osi na jeden typ czujnika\n",
        "N_AXES_SINGLE = 12\n",
        "N_AXES = 12 if SENSOR_MODE in (\"accel\", \"gyro\") else 24\n",
        "\n",
        "fft_dir = os.path.join(FFT_ROOT, FFT_CONFIG)"
      ],
      "metadata": {
        "id": "-3n9BPu7CQgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_fft_info(fft_dir, sampling_rate=500.0):\n",
        "    folder_name = os.path.basename(os.path.normpath(fft_dir))\n",
        "    parts = folder_name.split(\"_\")          # np. [\"128\",\"Hann\",\"20\",\"52\"]\n",
        "    measuringWindowLength = int(parts[0])   # 128\n",
        "    rangeStart = int(parts[-2])             # 20\n",
        "    rangeStop  = int(parts[-1])             # 52\n",
        "\n",
        "    freq_res = sampling_rate / measuringWindowLength\n",
        "    f_start  = (rangeStart - 1) * freq_res\n",
        "    f_stop   = rangeStop * freq_res\n",
        "\n",
        "    print(f\"Folder FFT: {folder_name}\")\n",
        "    print(f\"measuringWindowLength = {measuringWindowLength}\")\n",
        "    print(f\"Zakres binów: {rangeStart}–{rangeStop}\")\n",
        "    print(f\"Rozdzielczość częstotliwości: {freq_res:.3f} Hz\")\n",
        "    print(f\"Zakres częstotliwości: {f_start:.1f} Hz – {f_stop:.1f} Hz\")\n",
        "\n",
        "    return measuringWindowLength, rangeStart, rangeStop, freq_res, f_start, f_stop"
      ],
      "metadata": {
        "id": "DACo4DG-DI95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "measuringWindowLength, rangeStart, rangeStop, freq_res, f_start, f_stop = print_fft_info(fft_dir, SAMPLING_RATE)"
      ],
      "metadata": {
        "id": "Yfm-tRZ0BT6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_files = glob.glob(os.path.join(fft_dir, \"*.csv\"))\n",
        "\n",
        "accel_files = {}  # scenario -> ścieżka\n",
        "gyro_files  = {}\n",
        "\n",
        "for path in all_files:\n",
        "    fname = os.path.basename(path)\n",
        "    parts = fname.split(\"_\")\n",
        "    # przykład: Bebop2_16g_FFT_ACCEL_128_Hann_20_52_0000.csv\n",
        "    # indeksy:   0      1   2   3     4    5    6   7   8\n",
        "    sensor_type = parts[3]              # \"ACCEL\" albo \"GYRO\"\n",
        "    scenario    = os.path.splitext(parts[-1])[0]  # \"0000\" itd.\n",
        "\n",
        "    if sensor_type == \"ACCEL\":\n",
        "        accel_files[scenario] = path\n",
        "    elif sensor_type == \"GYRO\":\n",
        "        gyro_files[scenario] = path\n",
        "\n",
        "print(\"Znaleziono ACCEL dla scenariuszy:\", sorted(accel_files.keys()))\n",
        "print(\"Znaleziono GYRO  dla scenariuszy:\", sorted(gyro_files.keys()))\n"
      ],
      "metadata": {
        "id": "jeyN1TjjCrJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_fft_file(path, n_axes=N_AXES_SINGLE):\n",
        "    # jeśli okaże się, że plik ma nagłówek – zmień na header=0\n",
        "    df = pd.read_csv(path, header=None)\n",
        "    data = df.values.astype(\"float32\")   # (n_okien, n_features)\n",
        "    n_features = data.shape[1]\n",
        "\n",
        "    if n_features % n_axes != 0:\n",
        "        raise ValueError(f\"{os.path.basename(path)}: {n_features} kolumn \"\n",
        "                         f\"nie dzieli się przez {n_axes} osi.\")\n",
        "\n",
        "    n_freq_bins = n_features // n_axes\n",
        "    data_3d = data.reshape(-1, n_freq_bins, n_axes)  # (n_okien, n_freq_bins, n_axes)\n",
        "    return data_3d, n_freq_bins"
      ],
      "metadata": {
        "id": "0_NuRvTiCaAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_list = []\n",
        "y_list = []\n",
        "n_freq_bins_global = None\n",
        "\n",
        "for scenario, label in scenario_to_class.items():\n",
        "    cur_X = None\n",
        "\n",
        "    if SENSOR_MODE == \"accel\":\n",
        "        path = accel_files.get(scenario)\n",
        "        if path is None:\n",
        "            print(f\"[ACCEL] brak pliku dla scenariusza {scenario}, pomijam.\")\n",
        "            continue\n",
        "        accel_data, n_freq_bins = load_fft_file(path)\n",
        "        cur_X = accel_data   # (n_okien, n_freq_bins, 12)\n",
        "\n",
        "    elif SENSOR_MODE == \"gyro\":\n",
        "        path = gyro_files.get(scenario)\n",
        "        if path is None:\n",
        "            print(f\"[GYRO] brak pliku dla scenariusza {scenario}, pomijam.\")\n",
        "            continue\n",
        "        gyro_data, n_freq_bins = load_fft_file(path)\n",
        "        cur_X = gyro_data    # (n_okien, n_freq_bins, 12)\n",
        "\n",
        "    elif SENSOR_MODE == \"both\":\n",
        "        path_a = accel_files.get(scenario)\n",
        "        path_g = gyro_files.get(scenario)\n",
        "        if path_a is None or path_g is None:\n",
        "            print(f\"[BOTH] brak ACCEL lub GYRO dla {scenario}, pomijam.\")\n",
        "            continue\n",
        "\n",
        "        accel_data, n_freq_bins_a = load_fft_file(path_a, n_axes=N_AXES_SINGLE)\n",
        "        gyro_data,  n_freq_bins_g = load_fft_file(path_g, n_axes=N_AXES_SINGLE)\n",
        "\n",
        "        if accel_data.shape[0] != gyro_data.shape[0] or n_freq_bins_a != n_freq_bins_g:\n",
        "            raise ValueError(f\"Niezgodne rozmiary ACCEL/GYRO dla scenariusza {scenario}\")\n",
        "\n",
        "        # sklejanie po osi „kanałów”: 12 (ACCEL) + 12 (GYRO) = 24\n",
        "        cur_X = np.concatenate([accel_data, gyro_data], axis=-1)  # (..., n_freq_bins, 24)\n",
        "        n_freq_bins = n_freq_bins_a\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"SENSOR_MODE musi być 'accel', 'gyro' albo 'both'\")\n",
        "\n",
        "    # ustaw / sprawdź globalną liczbę binów\n",
        "    if n_freq_bins_global is None:\n",
        "        n_freq_bins_global = n_freq_bins\n",
        "    elif n_freq_bins_global != n_freq_bins:\n",
        "        raise ValueError(\"Różne n_freq_bins między plikami, coś jest nie tak.\")\n",
        "\n",
        "    X_list.append(cur_X)\n",
        "    y_list.append(np.full((cur_X.shape[0],), label, dtype=\"int32\"))\n",
        "\n",
        "# Sklejenie wszystkiego\n",
        "X = np.concatenate(X_list, axis=0)   # (N, n_freq_bins, N_AXES)\n",
        "y = np.concatenate(y_list, axis=0)   # (N,)\n",
        "\n",
        "print(\"X shape:\", X.shape)\n",
        "print(\"y shape:\", y.shape, \"unikalne etykiety:\", np.unique(y))\n",
        "\n",
        "n_freq_bins = n_freq_bins_global\n",
        "input_shape = (n_freq_bins, N_AXES)\n",
        "print(\"input_shape modelu:\", input_shape)\n"
      ],
      "metadata": {
        "id": "mqwyc0TwCjGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Models Creation"
      ],
      "metadata": {
        "id": "4l-eqQtTDsyN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.1 ANN - MLP"
      ],
      "metadata": {
        "id": "-qgCY9_PGXKD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize W&B Experiment, CONFIG\n",
        "wandb.init(\n",
        "    project=\"UAV-FDI-Bebop2\",\n",
        "    name=\"Freq_MLP\",\n",
        "    config={\n",
        "        \"domain\": \"freq\",\n",
        "        \"sensor_mode\": SENSOR_MODE,\n",
        "        \"input_shape\": input_shape,\n",
        "        \"output_shape\": NUM_CLASSES,\n",
        "        \"layer_1\": 120,\n",
        "        \"dropout_rate\": 0.5,\n",
        "        \"l2_value\": 0.01,\n",
        "        \"activation\": \"relu\",\n",
        "        \"final_activation\": \"softmax\",\n",
        "        \"optimizer\": \"adam\",\n",
        "        \"loss\": \"sparse_categorical_crossentropy\",\n",
        "        \"metric\": \"accuracy\",\n",
        "        \"learning_rate\": 0.001,\n",
        "        \"epochs\": 500,\n",
        "        \"batch_size\": 1000\n",
        "    }\n",
        ")\n",
        "\n",
        "\n",
        "config = wandb.config"
      ],
      "metadata": {
        "id": "VL33vW1bD-HV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G0KsHw0xGvPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Data division - train/val/test"
      ],
      "metadata": {
        "id": "6uXTYHDSG1wR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
        ")\n",
        "\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)) \\\n",
        "    .shuffle(buffer_size=len(X_train)) \\\n",
        "    .batch(config.batch_size) \\\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val)) \\\n",
        "    .batch(config.batch_size) \\\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)) \\\n",
        "    .batch(config.batch_size)"
      ],
      "metadata": {
        "id": "JyyRLJJcG1ET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Training Loop"
      ],
      "metadata": {
        "id": "tbfmWiycEi04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 3. TRENOWANIE\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"Rozpoczynam trenowanie...\")\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=config.epochs,\n",
        "    callbacks=[\n",
        "        # Loguje przebieg treningu (krzywe live)\n",
        "        WandbMetricsLogger(log_freq=5),\n",
        "        # Zapisuje najlepszy model (wymagane .keras dla Keras 3)\n",
        "        ModelCheckpoint(\n",
        "            filepath=\"checkpoints/model_best.keras\",\n",
        "            save_best_only=True,\n",
        "            monitor=\"val_accuracy\",\n",
        "            mode=\"max\",\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. EWALUACJA I OBLICZANIE METRYK\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\nGenerowanie predykcji na zbiorze testowym...\")\n",
        "y_true_list = []\n",
        "y_proba_list = []\n",
        "\n",
        "for x_batch, y_batch in test_ds:\n",
        "    y_true_list.append(y_batch.numpy())\n",
        "    y_proba_list.append(model.predict(x_batch, verbose=0))\n",
        "\n",
        "y_true = np.concatenate(y_true_list, axis=0)   # (N,)\n",
        "y_proba = np.concatenate(y_proba_list, axis=0) # (N, NUM_CLASSES)\n",
        "y_pred = np.argmax(y_proba, axis=1)            # (N,)\n",
        "\n",
        "# Upewniamy się, że y_true to inty\n",
        "if y_true.ndim > 1:\n",
        "    y_true_int = np.argmax(y_true, axis=1)\n",
        "else:\n",
        "    y_true_int = y_true.astype(int)\n",
        "\n",
        "# --- Metryki Liczbowe ---\n",
        "test_loss, test_acc = model.evaluate(test_ds, verbose=0)\n",
        "prec = precision_score(y_true_int, y_pred, average=\"macro\")\n",
        "rec = recall_score(y_true_int, y_pred, average=\"macro\")\n",
        "f1 = f1_score(y_true_int, y_pred, average=\"macro\")\n",
        "\n",
        "print(f\"Loss: {test_loss:.4f}, Acc: {test_acc:.4f}, F1: {f1:.4f}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 5. TWORZENIE WYKRESÓW (MATPLOTLIB)\n",
        "# ==============================================================================\n",
        "\n",
        "# A. Loss Plot\n",
        "fig_loss = plt.figure(figsize=(8, 5))\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "plt.title('Training Loss')\n",
        "plt.legend(); plt.grid(True)\n",
        "\n",
        "# B. Accuracy Plot\n",
        "fig_acc = plt.figure(figsize=(8, 5))\n",
        "plt.plot(history.history['accuracy'], label='Train Acc')\n",
        "plt.plot(history.history['val_accuracy'], label='Val Acc')\n",
        "plt.title('Training Accuracy')\n",
        "plt.legend(); plt.grid(True)\n",
        "\n",
        "# C. Confusion Matrix\n",
        "cm = confusion_matrix(y_true_int, y_pred)\n",
        "fig_cm, ax = plt.subplots(figsize=(8, 8))\n",
        "ConfusionMatrixDisplay(cm).plot(ax=ax, cmap='Blues', values_format='d')\n",
        "plt.title('Confusion Matrix')\n",
        "\n",
        "# D. ROC Curve\n",
        "y_true_bin = label_binarize(y_true_int, classes=np.arange(NUM_CLASSES))\n",
        "fpr, tpr, roc_auc = {}, {}, {}\n",
        "for i in range(NUM_CLASSES):\n",
        "    fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_proba[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "fig_roc = plt.figure(figsize=(10, 8))\n",
        "for i in range(NUM_CLASSES):\n",
        "    plt.plot(fpr[i], tpr[i], label=f'Cls {i} (AUC={roc_auc[i]:.2f})')\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.title('ROC Curve')\n",
        "# Legenda poza wykresem, żeby nie zasłaniała przy wielu klasach\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.tight_layout()\n",
        "\n",
        "# 2. RĘCZNE WYSŁANIE MODELU NA KOŃCU\n",
        "# Robimy to tylko raz, tuż przed wandb.finish()\n",
        "print(\"Wysyłanie najlepszego modelu do W&B...\")\n",
        "wandb.save(\"checkpoints/model_best.keras\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 6. LOGOWANIE FINALNE (Podział na Summary i Media)\n",
        "# ==============================================================================\n",
        "\n",
        "# 1. LICZBY -> wandb.summary (Tylko do tabeli, BEZ kropek na wykresach)\n",
        "wandb.summary[\"test_loss\"] = test_loss\n",
        "wandb.summary[\"test_accuracy\"] = test_acc\n",
        "wandb.summary[\"test_precision\"] = prec\n",
        "wandb.summary[\"test_recall\"] = rec\n",
        "wandb.summary[\"test_f1_score\"] = f1\n",
        "\n",
        "# 2. OBRAZKI -> wandb.log (Do sekcji Media / Custom Charts)\n",
        "wandb.log({\n",
        "    \"charts/loss_history\": wandb.Image(fig_loss),\n",
        "    \"charts/acc_history\": wandb.Image(fig_acc),\n",
        "    \"charts/confusion_matrix\": wandb.Image(fig_cm),\n",
        "    \"charts/roc_curve\": wandb.Image(fig_roc)\n",
        "})\n",
        "\n",
        "# Sprzątanie pamięci RAM (zamykamy wykresy matplotlib)\n",
        "plt.close('all')\n",
        "\n",
        "print(\"Proces zakończony. Wyniki wysłane do W&B.\")\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "uP-2eRM5Emgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Advanced Evaluation and Quality Metrics"
      ],
      "metadata": {
        "id": "HDElKgr_Hk-V"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m3DrDSJbHkKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Model Export\n",
        "\n",
        "To facilitate deployment on the drone's embedded platform or for future inference tasks, we serialize the trained model along with a metadata file. The `metadata.json` file contains critical configuration details regarding sensor setup and class mapping, which are essential for correctly interpreting model predictions in a production environment."
      ],
      "metadata": {
        "id": "tlD8ossoH7jX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "# katalog na zapisane modele\n",
        "SAVE_DIR = \"saved_models\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "MODEL_PATH    = os.path.join(SAVE_DIR, \"uav_fft_classifier.keras\")  # format Keras\n",
        "METADATA_PATH = os.path.join(SAVE_DIR, \"metadata.json\")\n",
        "\n",
        "# 1) zapis samego modelu\n",
        "model.save(MODEL_PATH)\n",
        "print(\"Zapisano model do:\", MODEL_PATH)\n",
        "\n",
        "# 2) zapis podstawowych metadanych (przydadzą się przy wczytywaniu)\n",
        "metadata = {\n",
        "    \"sensor_mode\": SENSOR_MODE,             # \"accel\" / \"gyro\" / \"both\"\n",
        "    \"num_classes\": int(NUM_CLASSES),\n",
        "    \"n_freq_bins\": int(n_freq_bins),\n",
        "    \"n_axes\": int(N_AXES),\n",
        "    \"scenario_to_class\": scenario_to_class, # mapping klas\n",
        "}\n",
        "\n",
        "with open(METADATA_PATH, \"w\") as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "\n",
        "print(\"Zapisano metadane do:\", METADATA_PATH)\n"
      ],
      "metadata": {
        "id": "-EW4rF3sH8Am"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}