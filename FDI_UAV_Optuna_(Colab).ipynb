{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM6yZjCMnmfUYm8pGgOAEsr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mateo755/UAV_ML_FDI/blob/main/FDI_UAV_Optuna_(Colab).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UAV Propeller Fault Detection System (Parrot Bebop 2)\n",
        "\n",
        "This project focuses on the development and validation of fault detection and isolation (FDI) methods for the propulsion system of the **Parrot Bebop 2** unmanned aerial vehicle (UAV). The analysis utilizes inertial sensor data (accelerometer and gyroscope) collected during real-world flight experiments.\n",
        "\n",
        "### Research Problem\n",
        "The primary objective is to classify the technical state of the propellers based on vibration signals. We analyze various fault scenarios across four rotors (A, B, C, D), distinguishing between nominal states and specific defects such as chipped edges or bent blades.\n",
        "\n",
        "### Methodology\n",
        "This notebook compares two signal processing approaches:\n",
        "1.  **Time Domain Analysis**\n",
        "2.  **Frequency Domain Analysis**\n",
        "\n",
        "Experiment tracking and performance visualization are managed via **Weights & Biases (W&B)**."
      ],
      "metadata": {
        "id": "ZPkfeiBvYMTH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Environment Setup & Global Configuration\n",
        "Installation of necessary libraries (Weights & Biases for experiment tracking) and importing standard data science modules."
      ],
      "metadata": {
        "id": "aAsXj0bQYPxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb\n",
        "!pip install optuna"
      ],
      "metadata": {
        "id": "wBuNU_VVYiyX",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "import optuna\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import glob\n",
        "import time\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.backend import clear_session\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Flatten, Dense, Dropout,\n",
        "    Conv1D, MaxPooling1D, GlobalAveragePooling1D, BatchNormalization\n",
        ")\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Adagrad, Adadelta, Adamax, Nadam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.utils import to_categorical, plot_model\n",
        "from tensorflow.keras.metrics import CategoricalAccuracy\n",
        "import traceback"
      ],
      "metadata": {
        "id": "W-l7qMBMYNZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "from wandb.integration.keras import WandbMetricsLogger\n",
        "wandb.login()"
      ],
      "metadata": {
        "id": "5rSoKrSzYm7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Exploratory Data Analysis\n",
        "Initial inspection of the accelerometer and gyroscope data from the UAV. We examine the column structure."
      ],
      "metadata": {
        "id": "Lzmgu-V0Ytww"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Time domain"
      ],
      "metadata": {
        "id": "dzm9tSDXY7ri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q /content/Normalized_data.zip"
      ],
      "metadata": {
        "id": "uNYX-PCdYuG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_time = pd.read_csv('/content/Normalized_data/Bebop2_16g_1kdps_normalized_0000.csv')\n",
        "df_time"
      ],
      "metadata": {
        "id": "tnvTK4vKYxBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_time.info()"
      ],
      "metadata": {
        "id": "3_Zi76BnYyR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Frequency Domain"
      ],
      "metadata": {
        "id": "mz_DOXEaY16a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q /content/FFT_data.zip"
      ],
      "metadata": {
        "id": "aLwfhGFKY55i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_freq = pd.read_csv('/content/FFT_data/128_Hann_20_52/Bebop2_16g_FFT_ACCEL_128_Hann_20_52_0000.csv', header=None)\n",
        "df_freq"
      ],
      "metadata": {
        "id": "G9bMctTgY96o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_freq.info()"
      ],
      "metadata": {
        "id": "0kb2qEQWY_Cy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Fault Scenario Mapping\n",
        "\n",
        "The Bebop 2 flight data is labeled using a 4-digit code (e.g., `1022`), defining the state of each propeller (A, B, C, D):\n",
        "* **0**: Nominal (Functional propeller).\n",
        "* **1**: Fault Type I (e.g., chipped edge).\n",
        "* **2**: Fault Type II (e.g., bent tip/severe damage).\n",
        "\n",
        "Below, we define the mapping of these physical scenarios to model class labels. Depending on the diagnostic granularity required, the problem can be framed as a **5-class problem** (aggregated by the number of faults) or a **20-class problem** (precise fault configuration)."
      ],
      "metadata": {
        "id": "uzoaYtfsZEi9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select Experiment Mode\n",
        "CLASS_MODE = \"20class\"   # Options: \"5class\" (aggregated) or \"20class\" (precise diagnosis)"
      ],
      "metadata": {
        "id": "_Ksm_yTMZE3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Precise mapping for 20 unique scenarios (Parrot Bebop 2)\n",
        "scenario_to_class_20 = {\n",
        "    \"0000\": 0,  # Nominal state\n",
        "    \"1000\": 1, \"0100\": 2, \"0010\": 3, \"0001\": 4,     # Single faults (Type 1)\n",
        "    \"2000\": 5, \"0200\": 6, \"0020\": 7, \"0002\": 8,     # Single faults (Type 2)\n",
        "    \"1100\": 9, \"1020\": 10, \"1002\": 11, \"0120\": 12, \"0102\": 13, \"0022\": 14, # Dual faults\n",
        "    \"1120\": 15, \"1102\": 16, \"1022\": 17, \"0122\": 18, # Triple faults\n",
        "    \"1122\": 19, # All propellers faulty\n",
        "}\n",
        "\n",
        "# Simplified mapping (Number of faulty rotors)\n",
        "scenario_to_class_5 = {\n",
        "    \"0000\": 0,\n",
        "    \"1000\": 1, \"0100\": 1, \"0010\": 1, \"0001\": 1,\n",
        "    \"2000\": 1, \"0200\": 1, \"0020\": 1, \"0002\": 1,\n",
        "    \"1100\": 2, \"1020\": 2, \"1002\": 2, \"0120\": 2, \"0102\": 2, \"0022\": 2,\n",
        "    \"1120\": 3, \"1102\": 3, \"1022\": 3, \"0122\": 3,\n",
        "    \"1122\": 4,\n",
        "}\n",
        "\n",
        "if CLASS_MODE == \"5class\":\n",
        "    scenario_to_class = scenario_to_class_5\n",
        "elif CLASS_MODE == \"20class\":\n",
        "    scenario_to_class = scenario_to_class_20\n",
        "else:\n",
        "    raise ValueError(\"Invalid CLASS_MODE selected.\")\n",
        "\n",
        "NUM_CLASSES = len(set(scenario_to_class.values()))\n",
        "print(f\"Experiment Mode: {CLASS_MODE} | Total Classes: {NUM_CLASSES}\")"
      ],
      "metadata": {
        "id": "LQntwDFzZH12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Time Domain Analysis, data preparation\n",
        "\n",
        "In this section, we process normalized time-series signals from the accelerometers and gyroscopes. Since the data represents a continuous flight stream, we apply a **sliding window** technique to segment the signal into fixed-length samples (e.g., 256 measurement points).\n",
        "\n"
      ],
      "metadata": {
        "id": "toqNSUaWZMFA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Data Segmentation"
      ],
      "metadata": {
        "id": "ksDaq9m4ZPS7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to normalized time-domain data\n",
        "DOMAIN = \"time\"\n",
        "DATA_DIR_TIME = r\"/content/Normalized_data\"\n",
        "SAMPLE_SIZE = 8               # Window length\n",
        "N_FEATURES = 24               # Input channels (e.g., 3-axis accel + 3-axis gyro per sensor)\n",
        "SENSOR_MODE = \"both\"          # \"accel\" + \"gyro\""
      ],
      "metadata": {
        "id": "nS4qmRjKZPFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_windows_from_df(df: pd.DataFrame, sample_size: int):\n",
        "    \"\"\"\n",
        "    Segments time-series data into non-overlapping windows.\n",
        "    \"\"\"\n",
        "    data = df.values.astype(\"float32\")\n",
        "    n_total = len(data)\n",
        "    n_windows = n_total // sample_size\n",
        "    if n_windows == 0:\n",
        "        return np.empty((0, sample_size, data.shape[1]), dtype=\"float32\")\n",
        "    data = data[:n_windows * sample_size]\n",
        "    windows = data.reshape(n_windows, sample_size, data.shape[1])\n",
        "    return windows"
      ],
      "metadata": {
        "id": "w2j0r69yZMcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Processing Pipeline\n",
        "The following loop iterates through all normalized CSV files in time domain. For each file, it:\n",
        "1.  Extracts the scenario code (e.g., `0000`) from the filename.\n",
        "2.  Checks if the scenario exists in our defined class mapping.\n",
        "3.  Loads the data and applies the sliding window segmentation.\n",
        "4.  Accumulates the processed windows (`X`) and corresponding labels (`y`) into a single dataset."
      ],
      "metadata": {
        "id": "zt6yDCdTZUBj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data pass through window function\n",
        "\n",
        "X_list = []\n",
        "y_list = []\n",
        "\n",
        "norm_data_files_pattern = os.path.join(DATA_DIR_TIME, \"Bebop2_16g_1kdps_normalized_*.csv\")\n",
        "\n",
        "for path in glob.glob(norm_data_files_pattern):\n",
        "    fname = os.path.basename(path)\n",
        "    # ostatni fragment po \"_\" to kod scenariusza, np. \"0000\"\n",
        "    scenario = os.path.splitext(fname)[0].split(\"_\")[-1]\n",
        "\n",
        "    if scenario not in scenario_to_class:\n",
        "        print(f\"Pomijam {fname} ‚Äì scenariusz {scenario} nie jest w mapowaniu.\")\n",
        "        continue\n",
        "\n",
        "    label = scenario_to_class[scenario]\n",
        "\n",
        "    df = pd.read_csv(path)\n",
        "\n",
        "    # je≈ºeli w pliku sƒÖ inne kolumny ni≈º 24 sensory, tu mo≈ºna wybraƒá tylko potrzebne:\n",
        "    # df = df[[\"A_aX\",\"A_aY\",...,\"D_gZ\"]]\n",
        "\n",
        "    windows = make_windows_from_df(df, SAMPLE_SIZE)\n",
        "    if windows.shape[0] == 0:\n",
        "        print(f\"Za ma≈Ço danych w {fname} na choƒá jedno okno, pomijam.\")\n",
        "        continue\n",
        "\n",
        "    X_list.append(windows)\n",
        "    y_list.append(np.full((windows.shape[0],), label, dtype=\"int32\"))\n",
        "\n",
        "X = np.concatenate(X_list, axis=0)  # (N, SAMPLE_SIZE, 24)\n",
        "y = np.concatenate(y_list, axis=0)  # (N,)\n",
        "\n",
        "print(\"X shape:\", X.shape)\n",
        "print(\"y shape:\", y.shape, \"unikalne etykiety:\", np.unique(y))\n",
        "\n",
        "input_shape = (SAMPLE_SIZE, N_FEATURES)\n",
        "print(\"input_shape modelu:\", input_shape)"
      ],
      "metadata": {
        "id": "Y-kvvG67ZUYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Frequency Domain Analysis (FFT)\n",
        "\n",
        "Mechanical faults in rotating components (such as propellers) generate distinct vibration signatures that are often most discernible in the frequency spectrum.\n",
        "\n",
        "In this experiment, we utilize data pre-processed via **Fast Fourier Transform (FFT)** using a Hann window to mitigate spectral leakage, which is already done in repo. The input features are vectors of spectral coefficients for each sensor axis."
      ],
      "metadata": {
        "id": "UoL2KH_XZXLp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 Spectral Data Structure\n",
        "The FFT files contain metadata within their filenames (window length, window type, frequency range). The following code parses this information and loads the corresponding spectral coefficients."
      ],
      "metadata": {
        "id": "bhEtio4xZY9I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration for FFT Data\n",
        "DOMAIN = \"fft\"\n",
        "FFT_ROOT      = \"FFT_data\"\n",
        "FFT_CONFIG    = \"128_Hann_20_52\"     # Specific window/range configuration\n",
        "SENSOR_MODE   = \"both\"               # \"accel\", \"gyro\", or \"both\"\n",
        "SAMPLING_RATE = 500.0                # Sampling rate for Bebop 2 inertial sensors\n",
        "\n",
        "# liczba osi na jeden typ czujnika\n",
        "N_AXES_SINGLE = 12\n",
        "N_AXES = 12 if SENSOR_MODE in (\"accel\", \"gyro\") else 24\n",
        "\n",
        "fft_dir = os.path.join(FFT_ROOT, FFT_CONFIG)"
      ],
      "metadata": {
        "id": "fOHXpGaTZXes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_fft_info(fft_dir, sampling_rate=500.0):\n",
        "    \"\"\"\n",
        "    Extracts FFT parameters encoded in the data folder name and converts them\n",
        "    into physical frequency values.\n",
        "\n",
        "    The function assumes the folder name follows the format:\n",
        "    'WindowLength_WindowType_StartBin_StopBin' (e.g., '128_Hann_20_52').\n",
        "    \"\"\"\n",
        "\n",
        "    folder_name = os.path.basename(os.path.normpath(fft_dir))\n",
        "    parts = folder_name.split(\"_\")          # np. [\"128\",\"Hann\",\"20\",\"52\"]\n",
        "    measuringWindowLength = int(parts[0])   # 128\n",
        "    rangeStart = int(parts[-2])             # 20\n",
        "    rangeStop  = int(parts[-1])             # 52\n",
        "\n",
        "    freq_res = sampling_rate / measuringWindowLength\n",
        "    f_start  = (rangeStart - 1) * freq_res\n",
        "    f_stop   = rangeStop * freq_res\n",
        "\n",
        "    print(f\"Folder FFT: {folder_name}\")\n",
        "    print(f\"measuringWindowLength = {measuringWindowLength}\")\n",
        "    print(f\"Zakres bin√≥w: {rangeStart}‚Äì{rangeStop}\")\n",
        "    print(f\"Rozdzielczo≈õƒá czƒôstotliwo≈õci: {freq_res:.3f} Hz\")\n",
        "    print(f\"Zakres czƒôstotliwo≈õci: {f_start:.1f} Hz ‚Äì {f_stop:.1f} Hz\")\n",
        "\n",
        "    return measuringWindowLength, rangeStart, rangeStop, freq_res, f_start, f_stop"
      ],
      "metadata": {
        "id": "p-yEwGWeZcfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "measuringWindowLength, rangeStart, rangeStop, freq_res, f_start, f_stop = print_fft_info(fft_dir, SAMPLING_RATE)"
      ],
      "metadata": {
        "id": "Qx84q7F_Zd4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### File Discovery & Categorization\n",
        "This block scans the directory for all CSV files and organizes them into dictionaries based on the sensor type (**ACCEL** vs. **GYRO**). It parses the filename to extract the specific fault scenario code (e.g., `0000`, `1022`), using it as a key for quick lookup during the data loading phase."
      ],
      "metadata": {
        "id": "Hm2eX53MZgvd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scan directory and map file paths to scenarios based on sensor type (ACCEL/GYRO)\n",
        "\n",
        "all_files = glob.glob(os.path.join(fft_dir, \"*.csv\"))\n",
        "\n",
        "accel_files = {}  # scenario -> ≈õcie≈ºka\n",
        "gyro_files  = {}\n",
        "\n",
        "for path in all_files:\n",
        "    fname = os.path.basename(path)\n",
        "    parts = fname.split(\"_\")\n",
        "    # przyk≈Çad: Bebop2_16g_FFT_ACCEL_128_Hann_20_52_0000.csv\n",
        "    # indeksy:   0      1   2   3     4    5    6   7   8\n",
        "    sensor_type = parts[3]              # \"ACCEL\" albo \"GYRO\"\n",
        "    scenario    = os.path.splitext(parts[-1])[0]  # \"0000\" itd.\n",
        "\n",
        "    if sensor_type == \"ACCEL\":\n",
        "        accel_files[scenario] = path\n",
        "    elif sensor_type == \"GYRO\":\n",
        "        gyro_files[scenario] = path\n",
        "\n",
        "print(\"Znaleziono ACCEL dla scenariuszy:\", sorted(accel_files.keys()))\n",
        "print(\"Znaleziono GYRO  dla scenariuszy:\", sorted(gyro_files.keys()))\n"
      ],
      "metadata": {
        "id": "T-KQMuUjZnI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Loading & Sensor Fusion\n",
        "In this step, we aggregate the spectral data based on the selected `SENSOR_MODE`.\n",
        "* **Accel/Gyro:** Loads only the specified sensor data.\n",
        "* **Both:** Loads both accelerometer and gyroscope files for the same scenario, verifying consistency, and concatenates them along the feature axis to create a unified feature vector (e.g., 24 input channels)."
      ],
      "metadata": {
        "id": "iK8NNsoiZkUL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_fft_file(path, n_axes=N_AXES_SINGLE):\n",
        "    \"\"\"\n",
        "    Loads spectral data from a CSV file and reshapes it into a 3D tensor.\n",
        "\n",
        "    The function reads a flat CSV (assuming no header), calculates the number\n",
        "    of frequency bins based on the total columns and specified axes, and\n",
        "    restructures the data.\n",
        "    \"\"\"\n",
        "\n",
        "    # je≈õli oka≈ºe siƒô, ≈ºe plik ma nag≈Ç√≥wek ‚Äì zmie≈Ñ na header=0\n",
        "    df = pd.read_csv(path, header=None)\n",
        "    data = df.values.astype(\"float32\")   # (n_okien, n_features)\n",
        "    n_features = data.shape[1]\n",
        "\n",
        "    if n_features % n_axes != 0:\n",
        "        raise ValueError(f\"{os.path.basename(path)}: {n_features} kolumn \"\n",
        "                         f\"nie dzieli siƒô przez {n_axes} osi.\")\n",
        "\n",
        "    n_freq_bins = n_features // n_axes\n",
        "    data_3d = data.reshape(-1, n_freq_bins, n_axes)  # (n_okien, n_freq_bins, n_axes)\n",
        "    return data_3d, n_freq_bins"
      ],
      "metadata": {
        "id": "291e8GCKZkjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_list = []\n",
        "y_list = []\n",
        "n_freq_bins_global = None\n",
        "\n",
        "for scenario, label in scenario_to_class.items():\n",
        "    cur_X = None\n",
        "\n",
        "    if SENSOR_MODE == \"accel\":\n",
        "        path = accel_files.get(scenario)\n",
        "        if path is None:\n",
        "            print(f\"[ACCEL] brak pliku dla scenariusza {scenario}, pomijam.\")\n",
        "            continue\n",
        "        accel_data, n_freq_bins = load_fft_file(path)\n",
        "        cur_X = accel_data   # (n_okien, n_freq_bins, 12)\n",
        "\n",
        "    elif SENSOR_MODE == \"gyro\":\n",
        "        path = gyro_files.get(scenario)\n",
        "        if path is None:\n",
        "            print(f\"[GYRO] brak pliku dla scenariusza {scenario}, pomijam.\")\n",
        "            continue\n",
        "        gyro_data, n_freq_bins = load_fft_file(path)\n",
        "        cur_X = gyro_data    # (n_okien, n_freq_bins, 12)\n",
        "\n",
        "    elif SENSOR_MODE == \"both\":\n",
        "        path_a = accel_files.get(scenario)\n",
        "        path_g = gyro_files.get(scenario)\n",
        "        if path_a is None or path_g is None:\n",
        "            print(f\"[BOTH] brak ACCEL lub GYRO dla {scenario}, pomijam.\")\n",
        "            continue\n",
        "\n",
        "        accel_data, n_freq_bins_a = load_fft_file(path_a, n_axes=N_AXES_SINGLE)\n",
        "        gyro_data,  n_freq_bins_g = load_fft_file(path_g, n_axes=N_AXES_SINGLE)\n",
        "\n",
        "        if accel_data.shape[0] != gyro_data.shape[0] or n_freq_bins_a != n_freq_bins_g:\n",
        "            raise ValueError(f\"Niezgodne rozmiary ACCEL/GYRO dla scenariusza {scenario}\")\n",
        "\n",
        "        # sklejanie po osi ‚Äûkana≈Ç√≥w‚Äù: 12 (ACCEL) + 12 (GYRO) = 24\n",
        "        cur_X = np.concatenate([accel_data, gyro_data], axis=-1)  # (..., n_freq_bins, 24)\n",
        "        n_freq_bins = n_freq_bins_a\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"SENSOR_MODE musi byƒá 'accel', 'gyro' albo 'both'\")\n",
        "\n",
        "    # ustaw / sprawd≈∫ globalnƒÖ liczbƒô bin√≥w\n",
        "    if n_freq_bins_global is None:\n",
        "        n_freq_bins_global = n_freq_bins\n",
        "    elif n_freq_bins_global != n_freq_bins:\n",
        "        raise ValueError(\"R√≥≈ºne n_freq_bins miƒôdzy plikami, co≈õ jest nie tak.\")\n",
        "\n",
        "    X_list.append(cur_X)\n",
        "    y_list.append(np.full((cur_X.shape[0],), label, dtype=\"int32\"))\n",
        "\n",
        "# Sklejenie wszystkiego\n",
        "X = np.concatenate(X_list, axis=0)   # (N, n_freq_bins, N_AXES)\n",
        "y = np.concatenate(y_list, axis=0)   # (N,)\n",
        "\n",
        "print(\"X shape:\", X.shape)\n",
        "print(\"y shape:\", y.shape, \"unikalne etykiety:\", np.unique(y))\n",
        "\n",
        "n_freq_bins = n_freq_bins_global\n",
        "input_shape = (n_freq_bins, N_AXES)\n",
        "print(\"input_shape modelu:\", input_shape)\n"
      ],
      "metadata": {
        "id": "10GDeEsEZpgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. OPTUNA"
      ],
      "metadata": {
        "id": "hIeFROpvZs8_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.1 MLP Model Builder"
      ],
      "metadata": {
        "id": "xnU06hZ3c6Yj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Cell: MLP Model Builder ---\n",
        "def MLPBuilder(optimizerStr, dropout, applyDropout, learningRate, hidden_layers_structure):\n",
        "    \"\"\"\n",
        "    Builds a Multi-Layer Perceptron (MLP) model dynamically.\n",
        "\n",
        "    Args:\n",
        "        optimizerStr (str): Name of the optimizer to use.\n",
        "        dropout (float): Dropout rate.\n",
        "        applyDropout (bool): Whether to apply dropout after dense layers.\n",
        "        learningRate (float): Learning rate for the optimizer.\n",
        "        hidden_layers_structure (list): List of integers defining the number of units in each dense layer.\n",
        "    \"\"\"\n",
        "    optimizerCls = {\n",
        "        \"Adam\": Adam, \"RMSprop\": RMSprop, \"SGD\": SGD,\n",
        "        \"Adagrad\": Adagrad, \"Adadelta\": Adadelta, \"Adamax\": Adamax, \"Nadam\": Nadam,\n",
        "    }\n",
        "\n",
        "    # 1. Input and Flattening\n",
        "    layers = [\n",
        "        Input(shape=input_shape),\n",
        "        Flatten()\n",
        "    ]\n",
        "\n",
        "    # 2. Dynamic Hidden Dense Layers\n",
        "    for units in hidden_layers_structure:\n",
        "        layers.append(Dense(units, activation=\"relu\"))\n",
        "        if applyDropout:\n",
        "            layers.append(Dropout(dropout))\n",
        "\n",
        "    # 3. Output Layer\n",
        "    layers.append(Dense(NUM_CLASSES, activation=\"softmax\"))\n",
        "\n",
        "    # 4. Model Assembly\n",
        "    model = Sequential(layers)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizerCls[optimizerStr](learning_rate=learningRate),\n",
        "        loss=\"categorical_crossentropy\",\n",
        "        metrics=[CategoricalAccuracy(name=\"accuracy\")]\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "DSfTG5E6ZuwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2 CNN (1D) Model Builder"
      ],
      "metadata": {
        "id": "XB74u1HcdjOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Cell: CNN Model Builder ---\n",
        "def CNNBuilder(optimizerStr, learningRate,\n",
        "               conv_layers_structure, kernel_size,\n",
        "               dense_layers_structure, dropout, applyDropout):\n",
        "    \"\"\"\n",
        "    Builds a 1D Convolutional Neural Network (CNN) model dynamically.\n",
        "\n",
        "    Args:\n",
        "        optimizerStr (str): Name of the optimizer.\n",
        "        learningRate (float): Learning rate.\n",
        "        conv_layers_structure (list): List of integers defining filters for each Conv1D layer.\n",
        "        kernel_size (int): Size of the 1D convolution window.\n",
        "        dense_layers_structure (list): List of integers for the dense layers after convolution.\n",
        "        dropout (float): Dropout rate for the dense layers.\n",
        "        applyDropout (bool): Whether to apply dropout in the dense block.\n",
        "    \"\"\"\n",
        "    optimizerCls = {\n",
        "        \"Adam\": Adam, \"RMSprop\": RMSprop, \"SGD\": SGD,\n",
        "        \"Adagrad\": Adagrad, \"Adadelta\": Adadelta, \"Adamax\": Adamax, \"Nadam\": Nadam,\n",
        "    }\n",
        "\n",
        "    layers = [Input(shape=input_shape)]\n",
        "\n",
        "    # 1. Dynamic Convolutional Blocks\n",
        "    # Each block consists of Conv1D -> MaxPooling1D\n",
        "    for filters in conv_layers_structure:\n",
        "        layers.append(Conv1D(filters=filters, kernel_size=kernel_size, activation='relu', padding='same'))\n",
        "        layers.append(MaxPooling1D(pool_size=2))\n",
        "        # Optional: You can add BatchNormalization() here if needed\n",
        "\n",
        "    # 2. Transition to Dense Layers\n",
        "    # GlobalAveragePooling1D is often better than Flatten for CNNs (reduces parameters significantly)\n",
        "    layers.append(GlobalAveragePooling1D())\n",
        "\n",
        "    # 3. Dynamic Dense Classification Head\n",
        "    for units in dense_layers_structure:\n",
        "        layers.append(Dense(units, activation='relu'))\n",
        "        if applyDropout:\n",
        "            layers.append(Dropout(dropout))\n",
        "\n",
        "    # 4. Output Layer\n",
        "    layers.append(Dense(NUM_CLASSES, activation=\"softmax\"))\n",
        "\n",
        "    # 5. Model Assembly\n",
        "    model = Sequential(layers)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizerCls[optimizerStr](learning_rate=learningRate),\n",
        "        loss=\"categorical_crossentropy\",\n",
        "        metrics=[CategoricalAccuracy(name=\"accuracy\")]\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "RvZYCSxXdiDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.3 Split data to train/val/test sets"
      ],
      "metadata": {
        "id": "2kNpUS7weD67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. Dane ---\n",
        "def CreateDataGenerators(batchSize, X, y):\n",
        "    if y.ndim == 1:\n",
        "        y_encoded = to_categorical(y, num_classes=NUM_CLASSES)\n",
        "    else:\n",
        "        y_encoded = y\n",
        "\n",
        "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "        X, y_encoded, test_size=0.3, random_state=42, stratify=y\n",
        "    )\n",
        "    y_stratify_temp = y[len(X_train):]\n",
        "\n",
        "    X_val, X_test, y_val, y_test = train_test_split(\n",
        "        X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_stratify_temp\n",
        "    )\n",
        "\n",
        "    train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)) \\\n",
        "        .shuffle(buffer_size=len(X_train)).batch(batchSize).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val)) \\\n",
        "        .batch(batchSize).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)) \\\n",
        "        .batch(batchSize)\n",
        "\n",
        "    return train_ds, val_ds, test_ds"
      ],
      "metadata": {
        "id": "6IXO77__eEaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.4 Optuna objective function"
      ],
      "metadata": {
        "id": "YxJnVhWDe1t4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#exp_name = \"Optuna_Hybrid_Search_FFT_v1\"\n",
        "\n",
        "exp_name = \"Optuna_Hybrid_Search_TIME_v2\""
      ],
      "metadata": {
        "id": "jC7GP62-lZxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ObjectiveFunction(trial):\n",
        "    # 1. Inicjalizacja W&B\n",
        "    parts = exp_name.split('_')\n",
        "\n",
        "    domain = parts[-2]  # \"TIME\"\n",
        "    version = parts[-1] # \"v2\"\n",
        "\n",
        "    run_name = f\"trial_{trial.number}_{domain}{version}\"\n",
        "\n",
        "    run = wandb.init(\n",
        "        project=\"UAV-FDI-Optimization\",\n",
        "        group=exp_name,\n",
        "        name=run_name,\n",
        "        job_type=\"hyperparam_opt\",\n",
        "        reinit=True\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        clear_session()\n",
        "\n",
        "        # --- A. Parametry Wsp√≥lne ---\n",
        "        # Te parametry sƒÖ u≈ºywane przez obie architektury\n",
        "\n",
        "        # --- A. Zapisywanie Konfiguracji Danych (METADANE) ---\n",
        "        # To zapisze siƒô w bazie, ale Optuna nie bƒôdzie tego \"losowaƒá\"\n",
        "        trial.set_user_attr(\"domain\", DOMAIN)\n",
        "        trial.set_user_attr(\"sensor_mode\", SENSOR_MODE)\n",
        "        trial.set_user_attr(\"input_shape\", str(input_shape)) # Warto zamieniƒá krotkƒô/listƒô na string dla bezpiecze≈Ñstwa bazy\n",
        "        trial.set_user_attr(\"output_shape\", NUM_CLASSES)\n",
        "\n",
        "        params = {\n",
        "            \"optimizer\": trial.suggest_categorical(\"optimizer\", [\"Adam\", \"Nadam\", \"RMSprop\", \"SGD\", \"Adagrad\"]),\n",
        "            \"learningRate\": trial.suggest_float(\"learningRate\", 1e-5, 1e-2, log=True),\n",
        "            \"batchSize\": trial.suggest_categorical(\"batchSize\", [64, 128, 256, 512]),\n",
        "            \"model_type\": trial.suggest_categorical(\"model_type\", [\"MLP\", \"CNN\"]), # Decyzja: kt√≥ra sieƒá?\n",
        "            \"epochs\": 15, # Sta≈Ça liczba epok dla por√≥wnania (mo≈ºesz zwiƒôkszyƒá)\n",
        "            \"domain\": DOMAIN,\n",
        "            \"sensor_mode\": SENSOR_MODE,\n",
        "            \"input_shape\": input_shape,\n",
        "            \"output_shape\": NUM_CLASSES,\n",
        "        }\n",
        "\n",
        "        model = None\n",
        "\n",
        "        # --- B. Rozga≈Çƒôzienie (Conditional Logic) ---\n",
        "\n",
        "        if params[\"model_type\"] == \"MLP\":\n",
        "            # === ≈öCIE≈ªKA MLP ===\n",
        "            dropout = trial.suggest_float(\"mlp_dropout\", 0.1, 0.5, step=0.1)\n",
        "            apply_dropout = trial.suggest_categorical(\"mlp_apply_dropout\", [True, False])\n",
        "\n",
        "            # Losowanie warstw Dense\n",
        "            n_layers = trial.suggest_int(\"mlp_n_layers\", 1, 4)\n",
        "            hidden_structure = []\n",
        "            for i in range(n_layers):\n",
        "                units = trial.suggest_int(f\"mlp_units_l{i}\", 32, 512, step=32)\n",
        "                hidden_structure.append(units)\n",
        "                params[f\"mlp_units_l{i}\"] = units # Logujemy do W&B\n",
        "\n",
        "            # Zapisujemy specyficzne parametry do s≈Çownika params\n",
        "            params.update({\n",
        "                \"dropout\": dropout,\n",
        "                \"applyDropout\": apply_dropout,\n",
        "                \"structure\": str(hidden_structure)\n",
        "            })\n",
        "\n",
        "            # Budujemy MLP\n",
        "            model = MLPBuilder(\n",
        "                optimizerStr=params[\"optimizer\"],\n",
        "                dropout=dropout,\n",
        "                applyDropout=apply_dropout,\n",
        "                learningRate=params[\"learningRate\"],\n",
        "                hidden_layers_structure=hidden_structure\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            # === ≈öCIE≈ªKA CNN ===\n",
        "            dropout = trial.suggest_float(\"cnn_dropout\", 0.1, 0.5, step=0.1)\n",
        "            apply_dropout = trial.suggest_categorical(\"cnn_apply_dropout\", [True, False])\n",
        "            kernel_size = trial.suggest_categorical(\"cnn_kernel_size\", [5, 10, 100, 500])\n",
        "\n",
        "            # 1. Losowanie warstw Conv\n",
        "            n_conv = trial.suggest_int(\"cnn_n_conv_layers\", 1, 4)\n",
        "            conv_structure = []\n",
        "            for i in range(n_conv):\n",
        "                filters = trial.suggest_int(f\"cnn_filters_l{i}\", 32, 512, step=32)\n",
        "                conv_structure.append(filters)\n",
        "                params[f\"cnn_filters_l{i}\"] = filters\n",
        "\n",
        "            # 2. Losowanie g≈Çowy Dense (Classification Head)\n",
        "            n_dense = trial.suggest_int(\"cnn_n_dense_head\", 1, 4)\n",
        "            dense_structure = []\n",
        "            for i in range(n_dense):\n",
        "                units = trial.suggest_int(f\"cnn_dense_l{i}\", 64, 512, step=32)\n",
        "                dense_structure.append(units)\n",
        "                params[f\"cnn_dense_l{i}\"] = units\n",
        "\n",
        "            # Zapisujemy parametry\n",
        "            params.update({\n",
        "                \"dropout\": dropout,\n",
        "                \"applyDropout\": apply_dropout,\n",
        "                \"kernel_size\": kernel_size,\n",
        "                \"conv_structure\": str(conv_structure),\n",
        "                \"dense_structure\": str(dense_structure)\n",
        "            })\n",
        "\n",
        "            # Budujemy CNN\n",
        "            model = CNNBuilder(\n",
        "                optimizerStr=params[\"optimizer\"],\n",
        "                learningRate=params[\"learningRate\"],\n",
        "                conv_layers_structure=conv_structure,\n",
        "                kernel_size=kernel_size,\n",
        "                dense_layers_structure=dense_structure,\n",
        "                dropout=dropout,\n",
        "                applyDropout=apply_dropout\n",
        "            )\n",
        "\n",
        "        # --- C. Logowanie Konfiguracji do W&B ---\n",
        "        wandb.config.update(params)\n",
        "\n",
        "        # --- D. Trening ---\n",
        "        train_ds, val_ds, test_ds = CreateDataGenerators(params[\"batchSize\"], X, y)\n",
        "\n",
        "        history = model.fit(\n",
        "            train_ds,\n",
        "            epochs=params[\"epochs\"],\n",
        "            validation_data=val_ds,\n",
        "            callbacks=[\n",
        "                EarlyStopping(patience=8, restore_best_weights=True),\n",
        "                WandbMetricsLogger(log_freq=\"epoch\") # Loguje loss/acc do W&B\n",
        "            ],\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        # --- E. Ewaluacja ---\n",
        "        loss, accuracy = model.evaluate(test_ds, verbose=0)\n",
        "\n",
        "        # Logujemy finalny wynik testowy\n",
        "        wandb.log({\"test_accuracy\": accuracy, \"test_loss\": loss})\n",
        "\n",
        "        wandb.finish()\n",
        "        return accuracy\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"!!! B≈ÇƒÖd w pr√≥bie {trial.number}: {e}\")\n",
        "        wandb.finish(exit_code=1)\n",
        "        return 0.0"
      ],
      "metadata": {
        "id": "G52-xfYdezCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.5 Optuna study start"
      ],
      "metadata": {
        "id": "3eozRvPhiCyj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Start Badania Hybrydowego ---\n",
        "db_folder = \"History/PretrainedOptuna\"\n",
        "os.makedirs(db_folder, exist_ok=True)\n",
        "storage_url = f\"sqlite:///{db_folder}/PretrainedOptuna.db\"\n",
        "\n",
        "# Nowa nazwa dla badania hybrydowego\n",
        "study_name = exp_name\n",
        "\n",
        "print(f\"Start badania: {study_name}\")\n",
        "study = optuna.create_study(\n",
        "    direction=\"maximize\",\n",
        "    study_name=study_name,\n",
        "    storage=storage_url,\n",
        "    load_if_exists=True\n",
        ")\n",
        "\n",
        "# Wy≈ÇƒÖczamy logi W&B w konsoli (≈ºeby by≈Ço czytelniej)\n",
        "os.environ[\"WANDB_SILENT\"] = \"true\"\n",
        "\n",
        "# Uruchamiamy np. 30 pr√≥b, ≈ºeby Optuna mia≈Ça czas przetestowaƒá oba typy\n",
        "study.optimize(ObjectiveFunction, n_trials=5, show_progress_bar=True)\n",
        "\n",
        "# --- Cell: Download Database to Local PC ---\n",
        "db_file = \"PretrainedOptuna.db\"\n",
        "db_path = os.path.join(db_folder, db_file)\n",
        "\n",
        "if os.path.exists(db_path):\n",
        "    print(f\"Pobieranie pliku: {db_path} ...\")\n",
        "    files.download(db_path)\n",
        "else:\n",
        "    print(f\"B≈ÇƒÖd: Plik {db_path} nie istnieje. Uruchom najpierw trening (Optunƒô).\")"
      ],
      "metadata": {
        "id": "4mFyuM-9iFZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Najlepsza pr√≥ba:\")\n",
        "print(f\"  Typ modelu: {study.best_params['model_type']}\")\n",
        "print(f\"  Accuracy: {study.best_trial.value}\")\n",
        "print(\"  Parametry:\", study.best_params)"
      ],
      "metadata": {
        "id": "kBE60UaYiVmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. SQL DATA"
      ],
      "metadata": {
        "id": "MwooImJUV6Ig"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.1 Download Database to Local PC"
      ],
      "metadata": {
        "id": "XRzbjTZsWGH1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Cell: Download Database to Local PC ---\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "# ≈öcie≈ºka do Twojej bazy (zdefiniowana wcze≈õniej w kodzie)\n",
        "db_folder = \"History/PretrainedOptuna\"\n",
        "db_file = \"PretrainedOptuna.db\"\n",
        "db_path = os.path.join(db_folder, db_file)\n",
        "\n",
        "if os.path.exists(db_path):\n",
        "    print(f\"Pobieranie pliku: {db_path} ...\")\n",
        "    files.download(db_path)\n",
        "else:\n",
        "    print(f\"B≈ÇƒÖd: Plik {db_path} nie istnieje. Uruchom najpierw trening (Optunƒô).\")"
      ],
      "metadata": {
        "id": "d6jS91fvV51i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.3 Upload Database from Local PC"
      ],
      "metadata": {
        "id": "giYyVFlZWKaG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Cell: Upload Database from Local PC ---\n",
        "import os\n",
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# 1. Przygotuj strukturƒô folder√≥w\n",
        "db_folder = \"History/PretrainedOptuna\"\n",
        "os.makedirs(db_folder, exist_ok=True)\n",
        "\n",
        "print(\"Wgraj plik 'PretrainedOptuna.db' ze swojego komputera:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# 2. Przenie≈õ wgrany plik do odpowiedniego folderu\n",
        "for filename in uploaded.keys():\n",
        "    if filename.endswith(\".db\"):\n",
        "        source_path = filename\n",
        "        destination_path = os.path.join(db_folder, filename)\n",
        "\n",
        "        # Przenoszenie (nadpisze plik, je≈õli ju≈º tam jest)\n",
        "        shutil.move(source_path, destination_path)\n",
        "        print(f\"Sukces! Baza danych przywr√≥cona do: {destination_path}\")\n",
        "        print(\"Mo≈ºesz teraz uruchomiƒá celƒô z ≈Çadowaniem Optuny (RUN_OPTIMIZATION = False).\")\n",
        "    else:\n",
        "        print(f\"Wgrano plik '{filename}', ale to nie wyglƒÖda na bazƒô danych (.db).\")"
      ],
      "metadata": {
        "id": "sebIHCqcV5iH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.3 SQLite data reading"
      ],
      "metadata": {
        "id": "CjOF6Hj7j23M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Database inspection"
      ],
      "metadata": {
        "id": "SL0Dsc1LhfRB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Cell: Inspect Database Content (Tables & Studies) ---\n",
        "import sqlite3\n",
        "import optuna\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Konfiguracja ≈õcie≈ºki\n",
        "db_folder = \"History/PretrainedOptuna\"\n",
        "db_file = \"PretrainedOptuna.db\"\n",
        "db_path = os.path.join(db_folder, db_file)\n",
        "storage_url = f\"sqlite:///{db_folder}/{db_file}\"\n",
        "\n",
        "if os.path.exists(db_path):\n",
        "    print(f\"üìÇ Analiza pliku bazy danych: {db_file}\\n\")\n",
        "\n",
        "    # --- CZƒò≈öƒÜ 1: Lista Tabel SQL (Techniczna struktura) ---\n",
        "    print(\"--- 1. Struktura Bazy Danych (Tabele SQL) ---\")\n",
        "    try:\n",
        "        conn = sqlite3.connect(db_path)\n",
        "        # Pobieramy nazwy tabel\n",
        "        query = \"SELECT name FROM sqlite_master WHERE type='table';\"\n",
        "        tables = pd.read_sql_query(query, conn)\n",
        "\n",
        "        if not tables.empty:\n",
        "            print(tables)\n",
        "            print(\"\\nWyja≈õnienie najwa≈ºniejszych tabel:\")\n",
        "            print(\" - studies: Lista Twoich eksperyment√≥w (np. v1, v2)\")\n",
        "            print(\" - trials: Lista wszystkich pr√≥b (run√≥w) ze wszystkich bada≈Ñ\")\n",
        "            print(\" - trial_values: Wyniki (Accuracy) dla ka≈ºdej pr√≥by\")\n",
        "            print(\" - trial_params: Parametry (lr, dropout) dla ka≈ºdej pr√≥by\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è Brak tabel. Baza jest pusta.\")\n",
        "        conn.close()\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå B≈ÇƒÖd SQL: {e}\")\n",
        "\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "# --- CZƒò≈öƒÜ 2: Lista Twoich Bada≈Ñ (Logiczna zawarto≈õƒá) ---\n",
        "    print(\"\\n--- 2. Zapisane Badania (Studies) ---\")\n",
        "    try:\n",
        "        # Optuna ma funkcjƒô do podsumowania wszystkich bada≈Ñ w pliku\n",
        "        summaries = optuna.get_all_study_summaries(storage=storage_url)\n",
        "\n",
        "        if summaries:\n",
        "            # Tworzymy ≈ÇadnƒÖ tabelkƒô\n",
        "            studies_data = []\n",
        "            for i, s in enumerate(summaries):\n",
        "                # POPRAWKA: U≈ºywamy enumerate zamiast s.study_id, kt√≥re zosta≈Ço usuniƒôte w nowszej Optunie\n",
        "                studies_data.append({\n",
        "                    \"Index\": i,\n",
        "                    \"Nazwa Badania\": s.study_name,\n",
        "                    \"Liczba Pr√≥b\": s.n_trials,\n",
        "                    \"Start\": s.datetime_start.strftime(\"%Y-%m-%d %H:%M\") if s.datetime_start else \"N/A\"\n",
        "                })\n",
        "\n",
        "            df_studies = pd.DataFrame(studies_data)\n",
        "            display(df_studies)\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è W tej bazie nie ma jeszcze ≈ºadnych bada≈Ñ.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå B≈ÇƒÖd Optuny: {e}\")\n",
        "\n",
        "else:\n",
        "    print(f\"‚ùå Plik {db_path} nie istnieje.\")"
      ],
      "metadata": {
        "id": "y0pr_ROugfYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Displaying experiments trials"
      ],
      "metadata": {
        "id": "RRBObVrchn6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- KONFIGURACJA ≈öCIE≈ªEK ---\n",
        "# Upewnij siƒô, ≈ºe ≈õcie≈ºka i nazwa badania sƒÖ identyczne jak w etapie treningu\n",
        "db_folder = \"History/PretrainedOptuna\"\n",
        "db_file = \"PretrainedOptuna.db\"\n",
        "storage_url = f\"sqlite:///{db_folder}/{db_file}\"\n",
        "study_name = exp_name\n",
        "\n",
        "# --- SPRAWDZENIE CZY BAZA ISTNIEJE ---\n",
        "if not os.path.exists(os.path.join(db_folder, db_file)):\n",
        "    print(f\"B≈ÅƒÑD: Nie znaleziono pliku bazy danych w: {db_folder}/{db_file}\")\n",
        "    print(\"Upewnij siƒô, ≈ºe uruchomi≈Çe≈õ wcze≈õniej trening.\")\n",
        "else:\n",
        "    print(f\"≈Åadowanie badania '{study_name}' z bazy danych...\")\n",
        "\n",
        "    # ≈Åadujemy istniejƒÖce badanie (nie tworzymy nowego)\n",
        "    try:\n",
        "        study = optuna.load_study(\n",
        "            study_name=study_name,\n",
        "            storage=storage_url\n",
        "        )\n",
        "\n",
        "        # --- 1. Wy≈õwietlenie Najlepszego Wyniku ---\n",
        "        if len(study.trials) > 0:\n",
        "            best_trial = study.best_trial\n",
        "            print(f\"\\nZnaleziono {len(study.trials)} zako≈Ñczonych pr√≥b.\")\n",
        "            print(f\"NAJLEPSZY WYNIK (Test Accuracy): {best_trial.value:.4f}\")\n",
        "            print(\"   Parametry zwyciƒôzcy:\")\n",
        "            for key, value in best_trial.params.items():\n",
        "                print(f\"     - {key}: {value}\")\n",
        "\n",
        "            # --- 2. Tabela TOP 5 Modeli ---\n",
        "            print(\"\\nTabela 5 Najlepszych Modeli:\")\n",
        "            df_results = study.trials_dataframe()\n",
        "\n",
        "            # Sortujemy malejƒÖco po wyniku (Accuracy)\n",
        "            df_top5 = df_results.sort_values(by='value', ascending=False).head(5)\n",
        "\n",
        "            # Lista kolumn, kt√≥re chcemy wy≈õwietliƒá (je≈õli istniejƒÖ w bazie)\n",
        "            # Optuna dodaje prefiks 'params_' do nazw parametr√≥w\n",
        "            wanted_cols = [\n",
        "                'number', 'value', 'params_model_type', 'duration'\n",
        "            ]\n",
        "\n",
        "            # Wybieramy tylko te kolumny, kt√≥re faktycznie sƒÖ w DataFrame\n",
        "            # (np. params_mlp_n_layers mo≈ºe nie istnieƒá, je≈õli wylosowano same CNN)\n",
        "            cols_to_show = [c for c in wanted_cols if c in df_top5.columns]\n",
        "\n",
        "            try:\n",
        "                display(df_top5[cols_to_show])\n",
        "            except NameError:\n",
        "                print(df_top5[cols_to_show].to_string())\n",
        "\n",
        "        else:\n",
        "            print(\"Badanie istnieje, ale nie zawiera ≈ºadnych zako≈Ñczonych pr√≥b.\")\n",
        "\n",
        "    except KeyError:\n",
        "        print(f\"Nie znaleziono badania o nazwie '{study_name}' w pliku .db.\")"
      ],
      "metadata": {
        "id": "s1Go4ew3j_1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_top5.columns"
      ],
      "metadata": {
        "id": "YmgFIfCMkLFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "study.study_name"
      ],
      "metadata": {
        "id": "EGEDo9AP1S3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Retrain Selected Model - W&B integration"
      ],
      "metadata": {
        "id": "ZtGH37KPkI1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === CONFIGURATION ===\n",
        "# Set to None to automatically select the best trial from the study.\n",
        "# Set to an integer (e.g., 5) to retrain a specific trial number.\n",
        "SELECTED_TRIAL_NUMBER = None\n",
        "\n",
        "# How many epochs to train for the final run (usually more than in search)\n",
        "RETRAIN_EPOCHS = 10\n",
        "# =====================\n",
        "\n",
        "# 1. Retrieve parameters\n",
        "if SELECTED_TRIAL_NUMBER is None:\n",
        "    target_trial = study.best_trial\n",
        "    print(f\"Selected BEST trial (ID: {target_trial.number}) with val_acc: {target_trial.value:.4f}\")\n",
        "else:\n",
        "    # Find trial by number\n",
        "    target_trial = next(t for t in study.trials if t.number == SELECTED_TRIAL_NUMBER)\n",
        "    print(f\"Selected specific trial (ID: {target_trial.number}) with val_acc: {target_trial.value:.4f}\")\n",
        "\n",
        "\n",
        "optuna_params = target_trial.params\n",
        "user_attributes = target_trial.user_attrs\n",
        "print(\"Parameters:\", optuna_params)\n",
        "\n",
        "# ≈ÅƒÖczymy parametry z Optuny z Twoimi sta≈Çymi parametrami\n",
        "combined_config = {\n",
        "    \"retrain_epochs\": RETRAIN_EPOCHS,\n",
        "    \"trial_id\": target_trial.number,\n",
        "    \"optuna_original_test_acc\": target_trial.value,\n",
        "    # Nazwa badania, z kt√≥rego pochodzi model\n",
        "    \"source_study_name\": study.study_name,\n",
        "    # Rozpakowanie parametr√≥w z Optuny:\n",
        "    **optuna_params,\n",
        "    **user_attributes\n",
        "}\n",
        "\n",
        "# 1. Inicjalizacja W&B dla finalnego treningu\n",
        "run = wandb.init(\n",
        "    project=\"UAV-FDI-Optimization\",   # Ten sam projekt co wcze≈õniej\n",
        "    group=\"Final_Training\",           # Nowa nazwa grupy (≈ºeby oddzieliƒá od searcha)\n",
        "    ## ----\n",
        "    name=\"Time_MLP_v1\",               # Unikalna nazwa tego konkretnego przebiegu\n",
        "    ## ----\n",
        "    config=combined_config,\n",
        "    reinit=True\n",
        ")\n",
        "\n",
        "#print(\"W&B zinicjalizowane. Config:\", combined_config)\n",
        "print(\"W&B zinicjalizowane.\")\n",
        "\n",
        "# --- 3. Reconstruct Architecture & Build Model ---\n",
        "model_type = optuna_params[\"model_type\"]\n",
        "final_model = None # Zmieniam nazwƒô na final_model dla porzƒÖdku\n",
        "\n",
        "if model_type == \"MLP\":\n",
        "    hidden_structure = []\n",
        "    n_layers = optuna_params[\"mlp_n_layers\"]\n",
        "    for i in range(n_layers):\n",
        "        hidden_structure.append(optuna_params[f\"mlp_units_l{i}\"])\n",
        "\n",
        "    print(f\"Building MLP with structure: {hidden_structure}\")\n",
        "\n",
        "    final_model = MLPBuilder(\n",
        "        optimizerStr=optuna_params[\"optimizer\"],\n",
        "        dropout=optuna_params[\"mlp_dropout\"],\n",
        "        applyDropout=optuna_params[\"mlp_apply_dropout\"],\n",
        "        learningRate=optuna_params[\"learningRate\"],\n",
        "        hidden_layers_structure=hidden_structure\n",
        "    )\n",
        "\n",
        "elif model_type == \"CNN\":\n",
        "    conv_structure = []\n",
        "    n_conv = optuna_params[\"cnn_n_conv_layers\"]\n",
        "    for i in range(n_conv):\n",
        "        conv_structure.append(optuna_params[f\"cnn_filters_l{i}\"])\n",
        "\n",
        "    dense_structure = []\n",
        "    n_dense = optuna_params[\"cnn_n_dense_head\"]\n",
        "    for i in range(n_dense):\n",
        "        dense_structure.append(optuna_params[f\"cnn_dense_l{i}\"])\n",
        "\n",
        "    print(f\"Building CNN with Conv: {conv_structure} and Dense: {dense_structure}\")\n",
        "\n",
        "    final_model = CNNBuilder(\n",
        "        optimizerStr=optuna_params[\"optimizer\"],\n",
        "        learningRate=optuna_params[\"learningRate\"],\n",
        "        conv_layers_structure=conv_structure,\n",
        "        kernel_size=optuna_params[\"cnn_kernel_size\"],\n",
        "        dense_layers_structure=dense_structure,\n",
        "        dropout=optuna_params[\"cnn_dropout\"],\n",
        "        applyDropout=optuna_params[\"cnn_apply_dropout\"]\n",
        "    )\n",
        "\n",
        "# 4. Prepare Data\n",
        "print(f\"Preparing data with Batch Size: {optuna_params['batchSize']}\")\n",
        "train_ds, val_ds, test_ds = CreateDataGenerators(optuna_params['batchSize'], X, y)\n",
        "\n",
        "# 5. Callbacks for Retraining\n",
        "callbacks_list = [\n",
        "    EarlyStopping(patience=15, restore_best_weights=True, monitor=\"val_accuracy\"),\n",
        "    ReduceLROnPlateau(factor=0.2, patience=10, min_lr=1e-6, monitor=\"val_loss\"),\n",
        "    WandbMetricsLogger(log_freq=\"epoch\")\n",
        "]\n",
        "\n",
        "\n",
        "# 6. Start Training\n",
        "print(f\"Rozpoczynam trening finalny ({RETRAIN_EPOCHS} epok)...\")\n",
        "\n",
        "history = final_model.fit(\n",
        "    train_ds,\n",
        "    epochs=RETRAIN_EPOCHS,\n",
        "    validation_data=val_ds,\n",
        "    callbacks=callbacks_list,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "\n",
        "# 7. Zapisz Model Checkpoint (Raz, po zako≈Ñczeniu)\n",
        "save_folder = \"final_models_ckpt\"\n",
        "os.makedirs(save_folder, exist_ok=True)\n",
        "model_filename = f\"{optuna_params[\"model_type\"]}_{user_attributes[\"domain\"]}_cl{user_attributes[\"output_shape\"]}_trial_{target_trial.number}_from_{study.study_name}.keras\"\n",
        "checkpoint_path = os.path.join(save_folder, model_filename)\n",
        "\n",
        "final_model.save(checkpoint_path)\n",
        "print(f\"Model zapisany lokalnie jako: {model_filename}\")\n",
        "\n",
        "artifact_name = f\"model_{optuna_params['model_type']}_{user_attributes['domain']}\"\n",
        "\n",
        "model_artifact = wandb.Artifact(\n",
        "    name=artifact_name,\n",
        "    type=\"model\",\n",
        "    #description=f\"Model\",\n",
        "    metadata=combined_config  # <--- Bardzo przydatne! Konfig jest przyklejony do modelu\n",
        ")\n",
        "\n",
        "\n",
        "model_artifact.add_file(checkpoint_path)\n",
        "\n",
        "\n",
        "run.log_artifact(model_artifact)\n",
        "\n",
        "print(f\"Model wys≈Çany jako Artifact: {artifact_name}\")\n",
        "\n",
        "# Wrzuƒá plik modelu do chmury W&B\n",
        "# wandb.save(model_filename)\n",
        "# print(\"Model wys≈Çany do Weights & Biases.\")\n",
        "\n",
        "# 4. Wygeneruj i wy≈õlij Schemat architektury\n",
        "plot_filename = \"model_architecture.png\"\n",
        "try:\n",
        "    # Tworzymy plik graficzny ze schematem\n",
        "    plot_model(\n",
        "        final_model,\n",
        "        to_file=plot_filename,\n",
        "        show_shapes=True,\n",
        "        show_layer_names=True,\n",
        "        expand_nested=True\n",
        "    )\n",
        "\n",
        "    # Logujemy obrazek do dashboardu\n",
        "    wandb.log({\"model_chart\": wandb.Image(plot_filename)})\n",
        "    print(\"Schemat architektury (plot_model) wys≈Çany do W&B.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Nie uda≈Ço siƒô wygenerowaƒá plot_model (mo≈ºe brakowaƒá graphviz): {e}\")\n",
        "\n",
        "# 5. Ewaluacja ko≈Ñcowa i zamkniƒôcie\n",
        "loss, accuracy = final_model.evaluate(test_ds, verbose=0)\n",
        "wandb.log({\"test_accuracy\": accuracy, \"test_loss\": loss})\n",
        "\n",
        "\n",
        "print(f\"Wynik ko≈Ñcowy na te≈õcie: {accuracy:.4f}\")\n",
        "\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "Mi-0Hgw_w3Gu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrain Selected Model - without online logger"
      ],
      "metadata": {
        "id": "7B50tBinpGfC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === CONFIGURATION ===\n",
        "# Set to None to automatically select the best trial from the study.\n",
        "# Set to an integer (e.g., 5) to retrain a specific trial number.\n",
        "SELECTED_TRIAL_NUMBER = 10\n",
        "\n",
        "# How many epochs to train for the final run (usually more than in search)\n",
        "RETRAIN_EPOCHS = 100\n",
        "# =====================\n",
        "\n",
        "# 1. Retrieve parameters\n",
        "if SELECTED_TRIAL_NUMBER is None:\n",
        "    target_trial = study.best_trial\n",
        "    print(f\"Selected BEST trial (ID: {target_trial.number}) with val_acc: {target_trial.value:.4f}\")\n",
        "else:\n",
        "    # Find trial by number\n",
        "    target_trial = next(t for t in study.trials if t.number == SELECTED_TRIAL_NUMBER)\n",
        "    print(f\"Selected specific trial (ID: {target_trial.number}) with val_acc: {target_trial.value:.4f}\")\n",
        "\n",
        "params = target_trial.params\n",
        "user_attributes = target_trial.user_attrs\n",
        "print(\"Parameters:\", params)\n",
        "\n",
        "# 2. Reconstruct Architecture & Build Model\n",
        "model_type = params[\"model_type\"]\n",
        "model = None\n",
        "\n",
        "if model_type == \"MLP\":\n",
        "    # --- Reconstruct MLP Structure ---\n",
        "    hidden_structure = []\n",
        "    n_layers = params[\"mlp_n_layers\"]\n",
        "    for i in range(n_layers):\n",
        "        hidden_structure.append(params[f\"mlp_units_l{i}\"])\n",
        "\n",
        "    print(f\"Building MLP with structure: {hidden_structure}\")\n",
        "\n",
        "    model = MLPBuilder(\n",
        "        optimizerStr=params[\"optimizer\"],\n",
        "        dropout=params[\"mlp_dropout\"],\n",
        "        applyDropout=params[\"mlp_apply_dropout\"],\n",
        "        learningRate=params[\"learningRate\"],\n",
        "        hidden_layers_structure=hidden_structure\n",
        "    )\n",
        "\n",
        "elif model_type == \"CNN\":\n",
        "    # --- Reconstruct CNN Structure ---\n",
        "    conv_structure = []\n",
        "    n_conv = params[\"cnn_n_conv_layers\"]\n",
        "    for i in range(n_conv):\n",
        "        conv_structure.append(params[f\"cnn_filters_l{i}\"])\n",
        "\n",
        "    dense_structure = []\n",
        "    n_dense = params[\"cnn_n_dense_head\"]\n",
        "    for i in range(n_dense):\n",
        "        dense_structure.append(params[f\"cnn_dense_l{i}\"])\n",
        "\n",
        "    print(f\"Building CNN with Conv: {conv_structure} and Dense: {dense_structure}\")\n",
        "\n",
        "    model = CNNBuilder(\n",
        "        optimizerStr=params[\"optimizer\"],\n",
        "        learningRate=params[\"learningRate\"],\n",
        "        conv_layers_structure=conv_structure,\n",
        "        kernel_size=params[\"cnn_kernel_size\"],\n",
        "        dense_layers_structure=dense_structure,\n",
        "        dropout=params[\"cnn_dropout\"],\n",
        "        applyDropout=params[\"cnn_apply_Dropout\"]\n",
        "    )\n",
        "\n",
        "# 3. Prepare Data (using the trial's batch size)\n",
        "print(f\"Preparing data with Batch Size: {params['batchSize']}\")\n",
        "train_ds, val_ds, test_ds = CreateDataGenerators(params['batchSize'], X, y)\n",
        "\n",
        "# 4. Callbacks for Retraining\n",
        "\n",
        "save_folder = \"final_models_ckpt\"\n",
        "os.makedirs(save_folder, exist_ok=True)\n",
        "model_filename = f\"{optuna_params[\"model_type\"]}_{user_attributes[\"domain\"]}_cl{user_attributes[\"output_shape\"]}_trial_{target_trial.number}_from_{study.study_name}.keras\"\n",
        "checkpoint_path = os.path.join(save_folder, model_filename)\n",
        "\n",
        "callbacks_list = [\n",
        "    ModelCheckpoint(checkpoint_path, save_best_only=True, monitor=\"val_accuracy\", mode=\"max\", verbose=1),\n",
        "    EarlyStopping(patience=8, restore_best_weights=True, monitor=\"val_accuracy\"),\n",
        "    ReduceLROnPlateau(factor=0.2, patience=3, min_lr=1e-6, monitor=\"val_loss\")\n",
        "]\n",
        "\n",
        "# 5. Start Training\n",
        "print(f\"\\nStarting retraining for {RETRAIN_EPOCHS} epochs...\")\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    epochs=RETRAIN_EPOCHS,\n",
        "    validation_data=val_ds,\n",
        "    callbacks=callbacks_list,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 6. Final Evaluation\n",
        "print(\"\\n--- Final Evaluation on Test Set ---\")\n",
        "loss, accuracy = model.evaluate(test_ds)\n",
        "print(f\"Final Test Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Model saved to: {checkpoint_path}\")\n",
        "\n",
        "# 7. Plotting Results\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Accuracy Plot\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
        "plt.title(f'Accuracy (Trial {target_trial.number})')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Loss Plot\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "plt.title(f'Loss (Trial {target_trial.number})')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Bpjpxi81pGFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model getting from w&b"
      ],
      "metadata": {
        "id": "t0m2KHW_Cijr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "import tensorflow as tf\n",
        "import glob\n",
        "\n",
        "run = wandb.init(project=\"UAV-FDI-Optimization\", job_type=\"inference\")\n",
        "\n",
        "# Pobierasz ZAWSZE najnowszƒÖ wersjƒô tego typu modelu:\n",
        "artifact = run.use_artifact('USER_NAME/UAV-FDI-Optimization/model_MLP_time:latest')\n",
        "\n",
        "\n",
        "# artifact.download() zwraca ≈õcie≈ºkƒô do folderu\n",
        "model_dir = artifact.download()\n",
        "\n",
        "# Szukamy dowolnego pliku ko≈ÑczƒÖcego siƒô na .keras w tym folderze\n",
        "files = glob.glob(os.path.join(model_dir, \"*.keras\"))\n",
        "\n",
        "if files:\n",
        "    model_path = files[0] # Bierzemy pierwszy znaleziony plik\n",
        "    print(f\"Znaleziono model: {model_path}\")\n",
        "    model = tf.keras.models.load_model(model_path)\n",
        "else:\n",
        "    raise FileNotFoundError(f\"Nie znaleziono pliku .keras w folderze {model_dir}\")"
      ],
      "metadata": {
        "id": "OigJEzISCgAH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}