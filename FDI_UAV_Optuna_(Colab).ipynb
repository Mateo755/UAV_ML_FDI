{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOUWYoTTlVR7ynfgG43R3Mz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mateo755/UAV_ML_FDI/blob/main/FDI_UAV_Optuna_(Colab).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UAV Propeller Fault Detection System (Parrot Bebop 2)\n",
        "\n",
        "This project focuses on the development and validation of fault detection and isolation (FDI) methods for the propulsion system of the **Parrot Bebop 2** unmanned aerial vehicle (UAV). The analysis utilizes inertial sensor data (accelerometer and gyroscope) collected during real-world flight experiments.\n",
        "\n",
        "### Research Problem\n",
        "The primary objective is to classify the technical state of the propellers based on vibration signals. We analyze various fault scenarios across four rotors (A, B, C, D), distinguishing between nominal states and specific defects such as chipped edges or bent blades.\n",
        "\n",
        "### Methodology\n",
        "This notebook compares two signal processing approaches:\n",
        "1.  **Time Domain Analysis**\n",
        "2.  **Frequency Domain Analysis**\n",
        "\n",
        "Experiment tracking and performance visualization are managed via **Weights & Biases (W&B)**."
      ],
      "metadata": {
        "id": "ZPkfeiBvYMTH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Environment Setup & Global Configuration\n",
        "Installation of necessary libraries (Weights & Biases for experiment tracking) and importing standard data science modules."
      ],
      "metadata": {
        "id": "aAsXj0bQYPxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb\n",
        "!pip install optuna"
      ],
      "metadata": {
        "id": "wBuNU_VVYiyX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "be09a7cb-87bb-4e62-cf06-416258d5cb74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (0.23.0)\n",
            "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb) (8.3.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (3.1.45)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from wandb) (25.0)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb) (4.5.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.12.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from wandb) (6.0.3)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.32.4)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.46.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.12/dist-packages (from wandb) (4.15.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2025.11.12)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Collecting optuna\n",
            "  Downloading optuna-4.6.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.17.2)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.44)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.3)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.3)\n",
            "Downloading optuna-4.6.0-py3-none-any.whl (404 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m404.7/404.7 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.10.1-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, optuna\n",
            "Successfully installed colorlog-6.10.1 optuna-4.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "import optuna\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import glob\n",
        "import time\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.backend import clear_session\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Flatten, Dense, Dropout,\n",
        "    Conv1D, MaxPooling1D, GlobalAveragePooling1D, BatchNormalization\n",
        ")\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Adagrad, Adadelta, Adamax, Nadam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.utils import to_categorical, plot_model\n",
        "from tensorflow.keras.metrics import CategoricalAccuracy\n",
        "import traceback"
      ],
      "metadata": {
        "id": "W-l7qMBMYNZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "from wandb.integration.keras import WandbMetricsLogger\n",
        "wandb.login()"
      ],
      "metadata": {
        "id": "5rSoKrSzYm7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Exploratory Data Analysis\n",
        "Initial inspection of the accelerometer and gyroscope data from the UAV. We examine the column structure."
      ],
      "metadata": {
        "id": "Lzmgu-V0Ytww"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Time domain"
      ],
      "metadata": {
        "id": "dzm9tSDXY7ri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q /content/Normalized_data.zip"
      ],
      "metadata": {
        "id": "uNYX-PCdYuG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_time = pd.read_csv('/content/Normalized_data/Bebop2_16g_1kdps_normalized_0000.csv')\n",
        "df_time"
      ],
      "metadata": {
        "id": "tnvTK4vKYxBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_time.info()"
      ],
      "metadata": {
        "id": "3_Zi76BnYyR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Frequency Domain"
      ],
      "metadata": {
        "id": "mz_DOXEaY16a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q /content/FFT_data.zip"
      ],
      "metadata": {
        "id": "aLwfhGFKY55i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_freq = pd.read_csv('/content/FFT_data/128_Hann_20_52/Bebop2_16g_FFT_ACCEL_128_Hann_20_52_0000.csv', header=None)\n",
        "df_freq"
      ],
      "metadata": {
        "id": "G9bMctTgY96o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_freq.info()"
      ],
      "metadata": {
        "id": "0kb2qEQWY_Cy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Fault Scenario Mapping\n",
        "\n",
        "The Bebop 2 flight data is labeled using a 4-digit code (e.g., `1022`), defining the state of each propeller (A, B, C, D):\n",
        "* **0**: Nominal (Functional propeller).\n",
        "* **1**: Fault Type I (e.g., chipped edge).\n",
        "* **2**: Fault Type II (e.g., bent tip/severe damage).\n",
        "\n",
        "Below, we define the mapping of these physical scenarios to model class labels. Depending on the diagnostic granularity required, the problem can be framed as a **5-class problem** (aggregated by the number of faults) or a **20-class problem** (precise fault configuration)."
      ],
      "metadata": {
        "id": "uzoaYtfsZEi9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select Experiment Mode\n",
        "CLASS_MODE = \"20class\"   # Options: \"5class\" (aggregated) or \"20class\" (precise diagnosis)"
      ],
      "metadata": {
        "id": "_Ksm_yTMZE3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Precise mapping for 20 unique scenarios (Parrot Bebop 2)\n",
        "scenario_to_class_20 = {\n",
        "    \"0000\": 0,  # Nominal state\n",
        "    \"1000\": 1, \"0100\": 2, \"0010\": 3, \"0001\": 4,     # Single faults (Type 1)\n",
        "    \"2000\": 5, \"0200\": 6, \"0020\": 7, \"0002\": 8,     # Single faults (Type 2)\n",
        "    \"1100\": 9, \"1020\": 10, \"1002\": 11, \"0120\": 12, \"0102\": 13, \"0022\": 14, # Dual faults\n",
        "    \"1120\": 15, \"1102\": 16, \"1022\": 17, \"0122\": 18, # Triple faults\n",
        "    \"1122\": 19, # All propellers faulty\n",
        "}\n",
        "\n",
        "# Simplified mapping (Number of faulty rotors)\n",
        "scenario_to_class_5 = {\n",
        "    \"0000\": 0,\n",
        "    \"1000\": 1, \"0100\": 1, \"0010\": 1, \"0001\": 1,\n",
        "    \"2000\": 1, \"0200\": 1, \"0020\": 1, \"0002\": 1,\n",
        "    \"1100\": 2, \"1020\": 2, \"1002\": 2, \"0120\": 2, \"0102\": 2, \"0022\": 2,\n",
        "    \"1120\": 3, \"1102\": 3, \"1022\": 3, \"0122\": 3,\n",
        "    \"1122\": 4,\n",
        "}\n",
        "\n",
        "if CLASS_MODE == \"5class\":\n",
        "    scenario_to_class = scenario_to_class_5\n",
        "elif CLASS_MODE == \"20class\":\n",
        "    scenario_to_class = scenario_to_class_20\n",
        "else:\n",
        "    raise ValueError(\"Invalid CLASS_MODE selected.\")\n",
        "\n",
        "NUM_CLASSES = len(set(scenario_to_class.values()))\n",
        "print(f\"Experiment Mode: {CLASS_MODE} | Total Classes: {NUM_CLASSES}\")"
      ],
      "metadata": {
        "id": "LQntwDFzZH12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0ecf3be-deb1-426d-bdc8-e3b800d7790f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experiment Mode: 20class | Total Classes: 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Time Domain Analysis, data preparation\n",
        "\n",
        "In this section, we process normalized time-series signals from the accelerometers and gyroscopes. Since the data represents a continuous flight stream, we apply a **sliding window** technique to segment the signal into fixed-length samples (e.g., 256 measurement points).\n",
        "\n"
      ],
      "metadata": {
        "id": "toqNSUaWZMFA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Data Segmentation"
      ],
      "metadata": {
        "id": "ksDaq9m4ZPS7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to normalized time-domain data\n",
        "DOMAIN = \"time\"\n",
        "DATA_DIR_TIME = r\"/content/Normalized_data\"\n",
        "SAMPLE_SIZE = 8               # Window length\n",
        "N_FEATURES = 24               # Input channels (e.g., 3-axis accel + 3-axis gyro per sensor)\n",
        "SENSOR_MODE = \"both\"          # \"accel\" + \"gyro\""
      ],
      "metadata": {
        "id": "nS4qmRjKZPFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_windows_from_df(df: pd.DataFrame, sample_size: int):\n",
        "    \"\"\"\n",
        "    Segments time-series data into non-overlapping windows.\n",
        "    \"\"\"\n",
        "    data = df.values.astype(\"float32\")\n",
        "    n_total = len(data)\n",
        "    n_windows = n_total // sample_size\n",
        "    if n_windows == 0:\n",
        "        return np.empty((0, sample_size, data.shape[1]), dtype=\"float32\")\n",
        "    data = data[:n_windows * sample_size]\n",
        "    windows = data.reshape(n_windows, sample_size, data.shape[1])\n",
        "    return windows"
      ],
      "metadata": {
        "id": "w2j0r69yZMcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Processing Pipeline\n",
        "The following loop iterates through all normalized CSV files in time domain. For each file, it:\n",
        "1.  Extracts the scenario code (e.g., `0000`) from the filename.\n",
        "2.  Checks if the scenario exists in our defined class mapping.\n",
        "3.  Loads the data and applies the sliding window segmentation.\n",
        "4.  Accumulates the processed windows (`X`) and corresponding labels (`y`) into a single dataset."
      ],
      "metadata": {
        "id": "zt6yDCdTZUBj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data pass through window function\n",
        "\n",
        "X_list = []\n",
        "y_list = []\n",
        "\n",
        "norm_data_files_pattern = os.path.join(DATA_DIR_TIME, \"Bebop2_16g_1kdps_normalized_*.csv\")\n",
        "\n",
        "for path in glob.glob(norm_data_files_pattern):\n",
        "    fname = os.path.basename(path)\n",
        "    # ostatni fragment po \"_\" to kod scenariusza, np. \"0000\"\n",
        "    scenario = os.path.splitext(fname)[0].split(\"_\")[-1]\n",
        "\n",
        "    if scenario not in scenario_to_class:\n",
        "        print(f\"Pomijam {fname} – scenariusz {scenario} nie jest w mapowaniu.\")\n",
        "        continue\n",
        "\n",
        "    label = scenario_to_class[scenario]\n",
        "\n",
        "    df = pd.read_csv(path)\n",
        "\n",
        "    # jeżeli w pliku są inne kolumny niż 24 sensory, tu można wybrać tylko potrzebne:\n",
        "    # df = df[[\"A_aX\",\"A_aY\",...,\"D_gZ\"]]\n",
        "\n",
        "    windows = make_windows_from_df(df, SAMPLE_SIZE)\n",
        "    if windows.shape[0] == 0:\n",
        "        print(f\"Za mało danych w {fname} na choć jedno okno, pomijam.\")\n",
        "        continue\n",
        "\n",
        "    X_list.append(windows)\n",
        "    y_list.append(np.full((windows.shape[0],), label, dtype=\"int32\"))\n",
        "\n",
        "X = np.concatenate(X_list, axis=0)  # (N, SAMPLE_SIZE, 24)\n",
        "y = np.concatenate(y_list, axis=0)  # (N,)\n",
        "\n",
        "print(\"X shape:\", X.shape)\n",
        "print(\"y shape:\", y.shape, \"unikalne etykiety:\", np.unique(y))\n",
        "\n",
        "input_shape = (SAMPLE_SIZE, N_FEATURES)\n",
        "print(\"input_shape modelu:\", input_shape)"
      ],
      "metadata": {
        "id": "Y-kvvG67ZUYv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7db9b83-8d91-4f36-92ba-9f7c395e0321"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X shape: (215040, 8, 24)\n",
            "y shape: (215040,) unikalne etykiety: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n",
            "input_shape modelu: (8, 24)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Frequency Domain Analysis (FFT)\n",
        "\n",
        "Mechanical faults in rotating components (such as propellers) generate distinct vibration signatures that are often most discernible in the frequency spectrum.\n",
        "\n",
        "In this experiment, we utilize data pre-processed via **Fast Fourier Transform (FFT)** using a Hann window to mitigate spectral leakage, which is already done in repo. The input features are vectors of spectral coefficients for each sensor axis."
      ],
      "metadata": {
        "id": "UoL2KH_XZXLp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 Spectral Data Structure\n",
        "The FFT files contain metadata within their filenames (window length, window type, frequency range). The following code parses this information and loads the corresponding spectral coefficients."
      ],
      "metadata": {
        "id": "bhEtio4xZY9I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration for FFT Data\n",
        "DOMAIN = \"fft\"\n",
        "FFT_ROOT      = \"FFT_data\"\n",
        "FFT_CONFIG    = \"128_Hann_20_52\"     # Specific window/range configuration\n",
        "SENSOR_MODE   = \"both\"               # \"accel\", \"gyro\", or \"both\"\n",
        "SAMPLING_RATE = 500.0                # Sampling rate for Bebop 2 inertial sensors\n",
        "\n",
        "# liczba osi na jeden typ czujnika\n",
        "N_AXES_SINGLE = 12\n",
        "N_AXES = 12 if SENSOR_MODE in (\"accel\", \"gyro\") else 24\n",
        "\n",
        "fft_dir = os.path.join(FFT_ROOT, FFT_CONFIG)"
      ],
      "metadata": {
        "id": "fOHXpGaTZXes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_fft_info(fft_dir, sampling_rate=500.0):\n",
        "    \"\"\"\n",
        "    Extracts FFT parameters encoded in the data folder name and converts them\n",
        "    into physical frequency values.\n",
        "\n",
        "    The function assumes the folder name follows the format:\n",
        "    'WindowLength_WindowType_StartBin_StopBin' (e.g., '128_Hann_20_52').\n",
        "    \"\"\"\n",
        "\n",
        "    folder_name = os.path.basename(os.path.normpath(fft_dir))\n",
        "    parts = folder_name.split(\"_\")          # np. [\"128\",\"Hann\",\"20\",\"52\"]\n",
        "    measuringWindowLength = int(parts[0])   # 128\n",
        "    rangeStart = int(parts[-2])             # 20\n",
        "    rangeStop  = int(parts[-1])             # 52\n",
        "\n",
        "    freq_res = sampling_rate / measuringWindowLength\n",
        "    f_start  = (rangeStart - 1) * freq_res\n",
        "    f_stop   = rangeStop * freq_res\n",
        "\n",
        "    print(f\"Folder FFT: {folder_name}\")\n",
        "    print(f\"measuringWindowLength = {measuringWindowLength}\")\n",
        "    print(f\"Zakres binów: {rangeStart}–{rangeStop}\")\n",
        "    print(f\"Rozdzielczość częstotliwości: {freq_res:.3f} Hz\")\n",
        "    print(f\"Zakres częstotliwości: {f_start:.1f} Hz – {f_stop:.1f} Hz\")\n",
        "\n",
        "    return measuringWindowLength, rangeStart, rangeStop, freq_res, f_start, f_stop"
      ],
      "metadata": {
        "id": "p-yEwGWeZcfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "measuringWindowLength, rangeStart, rangeStop, freq_res, f_start, f_stop = print_fft_info(fft_dir, SAMPLING_RATE)"
      ],
      "metadata": {
        "id": "Qx84q7F_Zd4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### File Discovery & Categorization\n",
        "This block scans the directory for all CSV files and organizes them into dictionaries based on the sensor type (**ACCEL** vs. **GYRO**). It parses the filename to extract the specific fault scenario code (e.g., `0000`, `1022`), using it as a key for quick lookup during the data loading phase."
      ],
      "metadata": {
        "id": "Hm2eX53MZgvd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scan directory and map file paths to scenarios based on sensor type (ACCEL/GYRO)\n",
        "\n",
        "all_files = glob.glob(os.path.join(fft_dir, \"*.csv\"))\n",
        "\n",
        "accel_files = {}  # scenario -> ścieżka\n",
        "gyro_files  = {}\n",
        "\n",
        "for path in all_files:\n",
        "    fname = os.path.basename(path)\n",
        "    parts = fname.split(\"_\")\n",
        "    # przykład: Bebop2_16g_FFT_ACCEL_128_Hann_20_52_0000.csv\n",
        "    # indeksy:   0      1   2   3     4    5    6   7   8\n",
        "    sensor_type = parts[3]              # \"ACCEL\" albo \"GYRO\"\n",
        "    scenario    = os.path.splitext(parts[-1])[0]  # \"0000\" itd.\n",
        "\n",
        "    if sensor_type == \"ACCEL\":\n",
        "        accel_files[scenario] = path\n",
        "    elif sensor_type == \"GYRO\":\n",
        "        gyro_files[scenario] = path\n",
        "\n",
        "print(\"Znaleziono ACCEL dla scenariuszy:\", sorted(accel_files.keys()))\n",
        "print(\"Znaleziono GYRO  dla scenariuszy:\", sorted(gyro_files.keys()))\n"
      ],
      "metadata": {
        "id": "T-KQMuUjZnI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Loading & Sensor Fusion\n",
        "In this step, we aggregate the spectral data based on the selected `SENSOR_MODE`.\n",
        "* **Accel/Gyro:** Loads only the specified sensor data.\n",
        "* **Both:** Loads both accelerometer and gyroscope files for the same scenario, verifying consistency, and concatenates them along the feature axis to create a unified feature vector (e.g., 24 input channels)."
      ],
      "metadata": {
        "id": "iK8NNsoiZkUL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_fft_file(path, n_axes=N_AXES_SINGLE):\n",
        "    \"\"\"\n",
        "    Loads spectral data from a CSV file and reshapes it into a 3D tensor.\n",
        "\n",
        "    The function reads a flat CSV (assuming no header), calculates the number\n",
        "    of frequency bins based on the total columns and specified axes, and\n",
        "    restructures the data.\n",
        "    \"\"\"\n",
        "\n",
        "    # jeśli okaże się, że plik ma nagłówek – zmień na header=0\n",
        "    df = pd.read_csv(path, header=None)\n",
        "    data = df.values.astype(\"float32\")   # (n_okien, n_features)\n",
        "    n_features = data.shape[1]\n",
        "\n",
        "    if n_features % n_axes != 0:\n",
        "        raise ValueError(f\"{os.path.basename(path)}: {n_features} kolumn \"\n",
        "                         f\"nie dzieli się przez {n_axes} osi.\")\n",
        "\n",
        "    n_freq_bins = n_features // n_axes\n",
        "    data_3d = data.reshape(-1, n_freq_bins, n_axes)  # (n_okien, n_freq_bins, n_axes)\n",
        "    return data_3d, n_freq_bins"
      ],
      "metadata": {
        "id": "291e8GCKZkjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_list = []\n",
        "y_list = []\n",
        "n_freq_bins_global = None\n",
        "\n",
        "for scenario, label in scenario_to_class.items():\n",
        "    cur_X = None\n",
        "\n",
        "    if SENSOR_MODE == \"accel\":\n",
        "        path = accel_files.get(scenario)\n",
        "        if path is None:\n",
        "            print(f\"[ACCEL] brak pliku dla scenariusza {scenario}, pomijam.\")\n",
        "            continue\n",
        "        accel_data, n_freq_bins = load_fft_file(path)\n",
        "        cur_X = accel_data   # (n_okien, n_freq_bins, 12)\n",
        "\n",
        "    elif SENSOR_MODE == \"gyro\":\n",
        "        path = gyro_files.get(scenario)\n",
        "        if path is None:\n",
        "            print(f\"[GYRO] brak pliku dla scenariusza {scenario}, pomijam.\")\n",
        "            continue\n",
        "        gyro_data, n_freq_bins = load_fft_file(path)\n",
        "        cur_X = gyro_data    # (n_okien, n_freq_bins, 12)\n",
        "\n",
        "    elif SENSOR_MODE == \"both\":\n",
        "        path_a = accel_files.get(scenario)\n",
        "        path_g = gyro_files.get(scenario)\n",
        "        if path_a is None or path_g is None:\n",
        "            print(f\"[BOTH] brak ACCEL lub GYRO dla {scenario}, pomijam.\")\n",
        "            continue\n",
        "\n",
        "        accel_data, n_freq_bins_a = load_fft_file(path_a, n_axes=N_AXES_SINGLE)\n",
        "        gyro_data,  n_freq_bins_g = load_fft_file(path_g, n_axes=N_AXES_SINGLE)\n",
        "\n",
        "        if accel_data.shape[0] != gyro_data.shape[0] or n_freq_bins_a != n_freq_bins_g:\n",
        "            raise ValueError(f\"Niezgodne rozmiary ACCEL/GYRO dla scenariusza {scenario}\")\n",
        "\n",
        "        # sklejanie po osi „kanałów”: 12 (ACCEL) + 12 (GYRO) = 24\n",
        "        cur_X = np.concatenate([accel_data, gyro_data], axis=-1)  # (..., n_freq_bins, 24)\n",
        "        n_freq_bins = n_freq_bins_a\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"SENSOR_MODE musi być 'accel', 'gyro' albo 'both'\")\n",
        "\n",
        "    # ustaw / sprawdź globalną liczbę binów\n",
        "    if n_freq_bins_global is None:\n",
        "        n_freq_bins_global = n_freq_bins\n",
        "    elif n_freq_bins_global != n_freq_bins:\n",
        "        raise ValueError(\"Różne n_freq_bins między plikami, coś jest nie tak.\")\n",
        "\n",
        "    X_list.append(cur_X)\n",
        "    y_list.append(np.full((cur_X.shape[0],), label, dtype=\"int32\"))\n",
        "\n",
        "# Sklejenie wszystkiego\n",
        "X = np.concatenate(X_list, axis=0)   # (N, n_freq_bins, N_AXES)\n",
        "y = np.concatenate(y_list, axis=0)   # (N,)\n",
        "\n",
        "print(\"X shape:\", X.shape)\n",
        "print(\"y shape:\", y.shape, \"unikalne etykiety:\", np.unique(y))\n",
        "\n",
        "n_freq_bins = n_freq_bins_global\n",
        "input_shape = (n_freq_bins, N_AXES)\n",
        "print(\"input_shape modelu:\", input_shape)\n"
      ],
      "metadata": {
        "id": "10GDeEsEZpgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. OPTUNA"
      ],
      "metadata": {
        "id": "hIeFROpvZs8_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.1 MLP Model Builder"
      ],
      "metadata": {
        "id": "xnU06hZ3c6Yj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Cell: MLP Model Builder ---\n",
        "def MLPBuilder(optimizerStr, dropout, applyDropout, learningRate, hidden_layers_structure):\n",
        "    \"\"\"\n",
        "    Builds a Multi-Layer Perceptron (MLP) model dynamically.\n",
        "\n",
        "    Args:\n",
        "        optimizerStr (str): Name of the optimizer to use.\n",
        "        dropout (float): Dropout rate.\n",
        "        applyDropout (bool): Whether to apply dropout after dense layers.\n",
        "        learningRate (float): Learning rate for the optimizer.\n",
        "        hidden_layers_structure (list): List of integers defining the number of units in each dense layer.\n",
        "    \"\"\"\n",
        "    optimizerCls = {\n",
        "        \"Adam\": Adam, \"RMSprop\": RMSprop, \"SGD\": SGD,\n",
        "        \"Adagrad\": Adagrad, \"Adadelta\": Adadelta, \"Adamax\": Adamax, \"Nadam\": Nadam,\n",
        "    }\n",
        "\n",
        "    # 1. Input and Flattening\n",
        "    layers = [\n",
        "        Input(shape=input_shape),\n",
        "        Flatten()\n",
        "    ]\n",
        "\n",
        "    # 2. Dynamic Hidden Dense Layers\n",
        "    for units in hidden_layers_structure:\n",
        "        layers.append(Dense(units, activation=\"relu\"))\n",
        "        if applyDropout:\n",
        "            layers.append(Dropout(dropout))\n",
        "\n",
        "    # 3. Output Layer\n",
        "    layers.append(Dense(NUM_CLASSES, activation=\"softmax\"))\n",
        "\n",
        "    # 4. Model Assembly\n",
        "    model = Sequential(layers)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizerCls[optimizerStr](learning_rate=learningRate),\n",
        "        loss=\"categorical_crossentropy\",\n",
        "        metrics=[CategoricalAccuracy(name=\"accuracy\")]\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "DSfTG5E6ZuwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2 CNN (1D) Model Builder"
      ],
      "metadata": {
        "id": "XB74u1HcdjOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Cell: CNN Model Builder ---\n",
        "def CNNBuilder(optimizerStr, learningRate,\n",
        "               conv_layers_structure, kernel_size,\n",
        "               dense_layers_structure, dropout, applyDropout):\n",
        "    \"\"\"\n",
        "    Builds a 1D Convolutional Neural Network (CNN) model dynamically.\n",
        "\n",
        "    Args:\n",
        "        optimizerStr (str): Name of the optimizer.\n",
        "        learningRate (float): Learning rate.\n",
        "        conv_layers_structure (list): List of integers defining filters for each Conv1D layer.\n",
        "        kernel_size (int): Size of the 1D convolution window.\n",
        "        dense_layers_structure (list): List of integers for the dense layers after convolution.\n",
        "        dropout (float): Dropout rate for the dense layers.\n",
        "        applyDropout (bool): Whether to apply dropout in the dense block.\n",
        "    \"\"\"\n",
        "    optimizerCls = {\n",
        "        \"Adam\": Adam, \"RMSprop\": RMSprop, \"SGD\": SGD,\n",
        "        \"Adagrad\": Adagrad, \"Adadelta\": Adadelta, \"Adamax\": Adamax, \"Nadam\": Nadam,\n",
        "    }\n",
        "\n",
        "    layers = [Input(shape=input_shape)]\n",
        "\n",
        "    # 1. Dynamic Convolutional Blocks\n",
        "    # Each block consists of Conv1D -> MaxPooling1D\n",
        "    for filters in conv_layers_structure:\n",
        "        layers.append(Conv1D(filters=filters, kernel_size=kernel_size, activation='relu', padding='same'))\n",
        "        layers.append(MaxPooling1D(pool_size=2))\n",
        "        # Optional: You can add BatchNormalization() here if needed\n",
        "\n",
        "    # 2. Transition to Dense Layers\n",
        "    # GlobalAveragePooling1D is often better than Flatten for CNNs (reduces parameters significantly)\n",
        "    layers.append(GlobalAveragePooling1D())\n",
        "\n",
        "    # 3. Dynamic Dense Classification Head\n",
        "    for units in dense_layers_structure:\n",
        "        layers.append(Dense(units, activation='relu'))\n",
        "        if applyDropout:\n",
        "            layers.append(Dropout(dropout))\n",
        "\n",
        "    # 4. Output Layer\n",
        "    layers.append(Dense(NUM_CLASSES, activation=\"softmax\"))\n",
        "\n",
        "    # 5. Model Assembly\n",
        "    model = Sequential(layers)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizerCls[optimizerStr](learning_rate=learningRate),\n",
        "        loss=\"categorical_crossentropy\",\n",
        "        metrics=[CategoricalAccuracy(name=\"accuracy\")]\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "RvZYCSxXdiDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.3 Split data to train/val/test sets"
      ],
      "metadata": {
        "id": "2kNpUS7weD67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. Dane ---\n",
        "def CreateDataGenerators(batchSize, X, y):\n",
        "    if y.ndim == 1:\n",
        "        y_encoded = to_categorical(y, num_classes=NUM_CLASSES)\n",
        "    else:\n",
        "        y_encoded = y\n",
        "\n",
        "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "        X, y_encoded, test_size=0.3, random_state=42, stratify=y\n",
        "    )\n",
        "    y_stratify_temp = y[len(X_train):]\n",
        "\n",
        "    X_val, X_test, y_val, y_test = train_test_split(\n",
        "        X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_stratify_temp\n",
        "    )\n",
        "\n",
        "    train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)) \\\n",
        "        .shuffle(buffer_size=len(X_train)).batch(batchSize).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val)) \\\n",
        "        .batch(batchSize).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)) \\\n",
        "        .batch(batchSize)\n",
        "\n",
        "    return train_ds, val_ds, test_ds"
      ],
      "metadata": {
        "id": "6IXO77__eEaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.4 Optuna objective function"
      ],
      "metadata": {
        "id": "YxJnVhWDe1t4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "exp_name = \"Optuna_Hybrid_Search_FFT_v1\"\n",
        "\n",
        "#exp_name = \"Optuna_Hybrid_Search_TIME_v1\""
      ],
      "metadata": {
        "id": "jC7GP62-lZxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ObjectiveFunction(trial):\n",
        "    # 1. Inicjalizacja W&B\n",
        "    run = wandb.init(\n",
        "        project=\"UAV-FDI-Optimization\",\n",
        "        group=exp_name,\n",
        "        name=f\"trial_{trial.number}\",\n",
        "        job_type=\"hyperparam_opt\",\n",
        "        reinit=True\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        clear_session()\n",
        "\n",
        "        # --- A. Parametry Wspólne ---\n",
        "        # Te parametry są używane przez obie architektury\n",
        "        params = {\n",
        "            \"optimizer\": trial.suggest_categorical(\"optimizer\", [\"Adam\", \"Nadam\", \"RMSprop\", \"SGD\", \"Adagrad\"]),\n",
        "            \"learningRate\": trial.suggest_float(\"learningRate\", 1e-5, 1e-2, log=True),\n",
        "            \"batchSize\": trial.suggest_categorical(\"batchSize\", [64, 128, 256, 512]),\n",
        "            \"model_type\": trial.suggest_categorical(\"model_type\", [\"MLP\", \"CNN\"]), # Decyzja: która sieć?\n",
        "            \"epochs\": 15, # Stała liczba epok dla porównania (możesz zwiększyć)\n",
        "            \"domain\": DOMAIN,\n",
        "            \"sensor_mode\": SENSOR_MODE,\n",
        "            \"input_shape\": input_shape,\n",
        "            \"output_shape\": NUM_CLASSES,\n",
        "        }\n",
        "\n",
        "        model = None\n",
        "\n",
        "        # --- B. Rozgałęzienie (Conditional Logic) ---\n",
        "\n",
        "        if params[\"model_type\"] == \"MLP\":\n",
        "            # === ŚCIEŻKA MLP ===\n",
        "            dropout = trial.suggest_float(\"mlp_dropout\", 0.1, 0.5, step=0.1)\n",
        "            apply_dropout = trial.suggest_categorical(\"mlp_apply_dropout\", [True, False])\n",
        "\n",
        "            # Losowanie warstw Dense\n",
        "            n_layers = trial.suggest_int(\"mlp_n_layers\", 1, 4)\n",
        "            hidden_structure = []\n",
        "            for i in range(n_layers):\n",
        "                units = trial.suggest_int(f\"mlp_units_l{i}\", 32, 512, step=32)\n",
        "                hidden_structure.append(units)\n",
        "                params[f\"mlp_units_l{i}\"] = units # Logujemy do W&B\n",
        "\n",
        "            # Zapisujemy specyficzne parametry do słownika params\n",
        "            params.update({\n",
        "                \"dropout\": dropout,\n",
        "                \"applyDropout\": apply_dropout,\n",
        "                \"structure\": str(hidden_structure)\n",
        "            })\n",
        "\n",
        "            # Budujemy MLP\n",
        "            model = MLPBuilder(\n",
        "                optimizerStr=params[\"optimizer\"],\n",
        "                dropout=dropout,\n",
        "                applyDropout=apply_dropout,\n",
        "                learningRate=params[\"learningRate\"],\n",
        "                hidden_layers_structure=hidden_structure\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            # === ŚCIEŻKA CNN ===\n",
        "            dropout = trial.suggest_float(\"cnn_dropout\", 0.1, 0.5, step=0.1)\n",
        "            apply_dropout = trial.suggest_categorical(\"cnn_apply_dropout\", [True, False])\n",
        "            kernel_size = trial.suggest_categorical(\"cnn_kernel_size\", [5, 10, 100, 500])\n",
        "\n",
        "            # 1. Losowanie warstw Conv\n",
        "            n_conv = trial.suggest_int(\"cnn_n_conv_layers\", 1, 4)\n",
        "            conv_structure = []\n",
        "            for i in range(n_conv):\n",
        "                filters = trial.suggest_int(f\"cnn_filters_l{i}\", 32, 512, step=32)\n",
        "                conv_structure.append(filters)\n",
        "                params[f\"cnn_filters_l{i}\"] = filters\n",
        "\n",
        "            # 2. Losowanie głowy Dense (Classification Head)\n",
        "            n_dense = trial.suggest_int(\"cnn_n_dense_head\", 1, 4)\n",
        "            dense_structure = []\n",
        "            for i in range(n_dense):\n",
        "                units = trial.suggest_int(f\"cnn_dense_l{i}\", 64, 512, step=32)\n",
        "                dense_structure.append(units)\n",
        "                params[f\"cnn_dense_l{i}\"] = units\n",
        "\n",
        "            # Zapisujemy parametry\n",
        "            params.update({\n",
        "                \"dropout\": dropout,\n",
        "                \"applyDropout\": apply_dropout,\n",
        "                \"kernel_size\": kernel_size,\n",
        "                \"conv_structure\": str(conv_structure),\n",
        "                \"dense_structure\": str(dense_structure)\n",
        "            })\n",
        "\n",
        "            # Budujemy CNN\n",
        "            model = CNNBuilder(\n",
        "                optimizerStr=params[\"optimizer\"],\n",
        "                learningRate=params[\"learningRate\"],\n",
        "                conv_layers_structure=conv_structure,\n",
        "                kernel_size=kernel_size,\n",
        "                dense_layers_structure=dense_structure,\n",
        "                dropout=dropout,\n",
        "                applyDropout=apply_dropout\n",
        "            )\n",
        "\n",
        "        # --- C. Logowanie Konfiguracji do W&B ---\n",
        "        wandb.config.update(params)\n",
        "\n",
        "        # --- D. Trening ---\n",
        "        train_ds, val_ds, test_ds = CreateDataGenerators(params[\"batchSize\"], X, y)\n",
        "\n",
        "        history = model.fit(\n",
        "            train_ds,\n",
        "            epochs=params[\"epochs\"],\n",
        "            validation_data=val_ds,\n",
        "            callbacks=[\n",
        "                EarlyStopping(patience=8, restore_best_weights=True),\n",
        "                WandbMetricsLogger(log_freq=\"epoch\") # Loguje loss/acc do W&B\n",
        "            ],\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        # --- E. Ewaluacja ---\n",
        "        loss, accuracy = model.evaluate(test_ds, verbose=0)\n",
        "\n",
        "        # Logujemy finalny wynik testowy\n",
        "        wandb.log({\"test_accuracy\": accuracy, \"test_loss\": loss})\n",
        "\n",
        "        wandb.finish()\n",
        "        return accuracy\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"!!! Błąd w próbie {trial.number}: {e}\")\n",
        "        wandb.finish(exit_code=1)\n",
        "        return 0.0"
      ],
      "metadata": {
        "id": "G52-xfYdezCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.5 Optuna study start"
      ],
      "metadata": {
        "id": "3eozRvPhiCyj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Start Badania Hybrydowego ---\n",
        "db_folder = \"History/PretrainedOptuna\"\n",
        "os.makedirs(db_folder, exist_ok=True)\n",
        "storage_url = f\"sqlite:///{db_folder}/PretrainedOptuna.db\"\n",
        "\n",
        "# Nowa nazwa dla badania hybrydowego\n",
        "study_name = exp_name\n",
        "\n",
        "print(f\"Start badania: {study_name}\")\n",
        "study = optuna.create_study(\n",
        "    direction=\"maximize\",\n",
        "    study_name=study_name,\n",
        "    storage=storage_url,\n",
        "    load_if_exists=True\n",
        ")\n",
        "\n",
        "# Wyłączamy logi W&B w konsoli (żeby było czytelniej)\n",
        "os.environ[\"WANDB_SILENT\"] = \"true\"\n",
        "\n",
        "# Uruchamiamy np. 30 prób, żeby Optuna miała czas przetestować oba typy\n",
        "study.optimize(ObjectiveFunction, n_trials=50, show_progress_bar=True)\n",
        "\n",
        "# --- Cell: Download Database to Local PC ---\n",
        "db_file = \"PretrainedOptuna.db\"\n",
        "db_path = os.path.join(db_folder, db_file)\n",
        "\n",
        "if os.path.exists(db_path):\n",
        "    print(f\"Pobieranie pliku: {db_path} ...\")\n",
        "    files.download(db_path)\n",
        "else:\n",
        "    print(f\"Błąd: Plik {db_path} nie istnieje. Uruchom najpierw trening (Optunę).\")"
      ],
      "metadata": {
        "id": "4mFyuM-9iFZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Najlepsza próba:\")\n",
        "print(f\"  Typ modelu: {study.best_params['model_type']}\")\n",
        "print(f\"  Accuracy: {study.best_trial.value}\")\n",
        "print(\"  Parametry:\", study.best_params)"
      ],
      "metadata": {
        "id": "kBE60UaYiVmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. SQL DATA"
      ],
      "metadata": {
        "id": "MwooImJUV6Ig"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.1 Download Database to Local PC"
      ],
      "metadata": {
        "id": "XRzbjTZsWGH1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Cell: Download Database to Local PC ---\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "# Ścieżka do Twojej bazy (zdefiniowana wcześniej w kodzie)\n",
        "db_folder = \"History/PretrainedOptuna\"\n",
        "db_file = \"PretrainedOptuna.db\"\n",
        "db_path = os.path.join(db_folder, db_file)\n",
        "\n",
        "if os.path.exists(db_path):\n",
        "    print(f\"Pobieranie pliku: {db_path} ...\")\n",
        "    files.download(db_path)\n",
        "else:\n",
        "    print(f\"Błąd: Plik {db_path} nie istnieje. Uruchom najpierw trening (Optunę).\")"
      ],
      "metadata": {
        "id": "d6jS91fvV51i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "49e246d9-2704-45ac-fd7a-6db54b534385"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pobieranie pliku: History/PretrainedOptuna/PretrainedOptuna.db ...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_6b5ad3d3-94e5-421c-b0b7-892aa808b841\", \"PretrainedOptuna.db\", 159744)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.3 Upload Database from Local PC"
      ],
      "metadata": {
        "id": "giYyVFlZWKaG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Cell: Upload Database from Local PC ---\n",
        "import os\n",
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# 1. Przygotuj strukturę folderów\n",
        "db_folder = \"History/PretrainedOptuna\"\n",
        "os.makedirs(db_folder, exist_ok=True)\n",
        "\n",
        "print(\"Wgraj plik 'PretrainedOptuna.db' ze swojego komputera:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# 2. Przenieś wgrany plik do odpowiedniego folderu\n",
        "for filename in uploaded.keys():\n",
        "    if filename.endswith(\".db\"):\n",
        "        source_path = filename\n",
        "        destination_path = os.path.join(db_folder, filename)\n",
        "\n",
        "        # Przenoszenie (nadpisze plik, jeśli już tam jest)\n",
        "        shutil.move(source_path, destination_path)\n",
        "        print(f\"Sukces! Baza danych przywrócona do: {destination_path}\")\n",
        "        print(\"Możesz teraz uruchomić celę z ładowaniem Optuny (RUN_OPTIMIZATION = False).\")\n",
        "    else:\n",
        "        print(f\"Wgrano plik '{filename}', ale to nie wygląda na bazę danych (.db).\")"
      ],
      "metadata": {
        "id": "sebIHCqcV5iH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.3 SQLite data reading"
      ],
      "metadata": {
        "id": "CjOF6Hj7j23M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Database inspection"
      ],
      "metadata": {
        "id": "SL0Dsc1LhfRB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Cell: Inspect Database Content (Tables & Studies) ---\n",
        "import sqlite3\n",
        "import optuna\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Konfiguracja ścieżki\n",
        "db_folder = \"History/PretrainedOptuna\"\n",
        "db_file = \"PretrainedOptuna.db\"\n",
        "db_path = os.path.join(db_folder, db_file)\n",
        "storage_url = f\"sqlite:///{db_folder}/{db_file}\"\n",
        "\n",
        "if os.path.exists(db_path):\n",
        "    print(f\"📂 Analiza pliku bazy danych: {db_file}\\n\")\n",
        "\n",
        "    # --- CZĘŚĆ 1: Lista Tabel SQL (Techniczna struktura) ---\n",
        "    print(\"--- 1. Struktura Bazy Danych (Tabele SQL) ---\")\n",
        "    try:\n",
        "        conn = sqlite3.connect(db_path)\n",
        "        # Pobieramy nazwy tabel\n",
        "        query = \"SELECT name FROM sqlite_master WHERE type='table';\"\n",
        "        tables = pd.read_sql_query(query, conn)\n",
        "\n",
        "        if not tables.empty:\n",
        "            print(tables)\n",
        "            print(\"\\nWyjaśnienie najważniejszych tabel:\")\n",
        "            print(\" - studies: Lista Twoich eksperymentów (np. v1, v2)\")\n",
        "            print(\" - trials: Lista wszystkich prób (runów) ze wszystkich badań\")\n",
        "            print(\" - trial_values: Wyniki (Accuracy) dla każdej próby\")\n",
        "            print(\" - trial_params: Parametry (lr, dropout) dla każdej próby\")\n",
        "        else:\n",
        "            print(\"⚠️ Brak tabel. Baza jest pusta.\")\n",
        "        conn.close()\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Błąd SQL: {e}\")\n",
        "\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "# --- CZĘŚĆ 2: Lista Twoich Badań (Logiczna zawartość) ---\n",
        "    print(\"\\n--- 2. Zapisane Badania (Studies) ---\")\n",
        "    try:\n",
        "        # Optuna ma funkcję do podsumowania wszystkich badań w pliku\n",
        "        summaries = optuna.get_all_study_summaries(storage=storage_url)\n",
        "\n",
        "        if summaries:\n",
        "            # Tworzymy ładną tabelkę\n",
        "            studies_data = []\n",
        "            for i, s in enumerate(summaries):\n",
        "                # POPRAWKA: Używamy enumerate zamiast s.study_id, które zostało usunięte w nowszej Optunie\n",
        "                studies_data.append({\n",
        "                    \"Index\": i,\n",
        "                    \"Nazwa Badania\": s.study_name,\n",
        "                    \"Liczba Prób\": s.n_trials,\n",
        "                    \"Start\": s.datetime_start.strftime(\"%Y-%m-%d %H:%M\") if s.datetime_start else \"N/A\"\n",
        "                })\n",
        "\n",
        "            df_studies = pd.DataFrame(studies_data)\n",
        "            display(df_studies)\n",
        "        else:\n",
        "            print(\"⚠️ W tej bazie nie ma jeszcze żadnych badań.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Błąd Optuny: {e}\")\n",
        "\n",
        "else:\n",
        "    print(f\"❌ Plik {db_path} nie istnieje.\")"
      ],
      "metadata": {
        "id": "y0pr_ROugfYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Displaying experiments trials"
      ],
      "metadata": {
        "id": "RRBObVrchn6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- KONFIGURACJA ŚCIEŻEK ---\n",
        "# Upewnij się, że ścieżka i nazwa badania są identyczne jak w etapie treningu\n",
        "db_folder = \"History/PretrainedOptuna\"\n",
        "db_file = \"PretrainedOptuna.db\"\n",
        "storage_url = f\"sqlite:///{db_folder}/{db_file}\"\n",
        "study_name = \"Optuna_Hybrid_MLP_CNN_Time_v1\"\n",
        "\n",
        "# --- SPRAWDZENIE CZY BAZA ISTNIEJE ---\n",
        "if not os.path.exists(os.path.join(db_folder, db_file)):\n",
        "    print(f\"BŁĄD: Nie znaleziono pliku bazy danych w: {db_folder}/{db_file}\")\n",
        "    print(\"Upewnij się, że uruchomiłeś wcześniej trening.\")\n",
        "else:\n",
        "    print(f\"Ładowanie badania '{study_name}' z bazy danych...\")\n",
        "\n",
        "    # Ładujemy istniejące badanie (nie tworzymy nowego)\n",
        "    try:\n",
        "        study = optuna.load_study(\n",
        "            study_name=study_name,\n",
        "            storage=storage_url\n",
        "        )\n",
        "\n",
        "        # --- 1. Wyświetlenie Najlepszego Wyniku ---\n",
        "        if len(study.trials) > 0:\n",
        "            best_trial = study.best_trial\n",
        "            print(f\"\\nZnaleziono {len(study.trials)} zakończonych prób.\")\n",
        "            print(f\"NAJLEPSZY WYNIK (Test Accuracy): {best_trial.value:.4f}\")\n",
        "            print(\"   Parametry zwycięzcy:\")\n",
        "            for key, value in best_trial.params.items():\n",
        "                print(f\"     - {key}: {value}\")\n",
        "\n",
        "            # --- 2. Tabela TOP 5 Modeli ---\n",
        "            print(\"\\nTabela 5 Najlepszych Modeli:\")\n",
        "            df_results = study.trials_dataframe()\n",
        "\n",
        "            # Sortujemy malejąco po wyniku (Accuracy)\n",
        "            df_top5 = df_results.sort_values(by='value', ascending=False).head(5)\n",
        "\n",
        "            # Lista kolumn, które chcemy wyświetlić (jeśli istnieją w bazie)\n",
        "            # Optuna dodaje prefiks 'params_' do nazw parametrów\n",
        "            wanted_cols = [\n",
        "                'number', 'value', 'params_model_type', 'duration'\n",
        "            ]\n",
        "\n",
        "            # Wybieramy tylko te kolumny, które faktycznie są w DataFrame\n",
        "            # (np. params_mlp_n_layers może nie istnieć, jeśli wylosowano same CNN)\n",
        "            cols_to_show = [c for c in wanted_cols if c in df_top5.columns]\n",
        "\n",
        "            try:\n",
        "                display(df_top5[cols_to_show])\n",
        "            except NameError:\n",
        "                print(df_top5[cols_to_show].to_string())\n",
        "\n",
        "        else:\n",
        "            print(\"Badanie istnieje, ale nie zawiera żadnych zakończonych prób.\")\n",
        "\n",
        "    except KeyError:\n",
        "        print(f\"Nie znaleziono badania o nazwie '{study_name}' w pliku .db.\")"
      ],
      "metadata": {
        "id": "s1Go4ew3j_1m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "outputId": "a4fe1587-4e67-4b89-ad6f-38dcd8324a77"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ładowanie badania 'Optuna_Hybrid_MLP_CNN_Time_v1' z bazy danych...\n",
            "\n",
            "Znaleziono 30 zakończonych prób.\n",
            "NAJLEPSZY WYNIK (Test Accuracy): 0.9397\n",
            "   Parametry zwycięzcy:\n",
            "     - optimizer: Nadam\n",
            "     - learningRate: 0.001881677109162985\n",
            "     - batchSize: 64\n",
            "     - model_type: MLP\n",
            "     - mlp_dropout: 0.5\n",
            "     - mlp_apply_dropout: True\n",
            "     - mlp_n_layers: 1\n",
            "     - mlp_units_l0: 416\n",
            "\n",
            "Tabela 5 Najlepszych Modeli:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "    number     value params_model_type               duration\n",
              "13      13  0.939664               MLP 0 days 00:00:13.746891\n",
              "10      10  0.935707               MLP 0 days 00:00:13.536338\n",
              "11      11  0.934718               MLP 0 days 00:00:12.305212\n",
              "17      17  0.932740               MLP 0 days 00:00:11.545463\n",
              "21      21  0.932740               MLP 0 days 00:00:12.920959"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2c11102a-ddb8-4b08-bff4-acb6e88b6243\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>number</th>\n",
              "      <th>value</th>\n",
              "      <th>params_model_type</th>\n",
              "      <th>duration</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>13</td>\n",
              "      <td>0.939664</td>\n",
              "      <td>MLP</td>\n",
              "      <td>0 days 00:00:13.746891</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>10</td>\n",
              "      <td>0.935707</td>\n",
              "      <td>MLP</td>\n",
              "      <td>0 days 00:00:13.536338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>11</td>\n",
              "      <td>0.934718</td>\n",
              "      <td>MLP</td>\n",
              "      <td>0 days 00:00:12.305212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>17</td>\n",
              "      <td>0.932740</td>\n",
              "      <td>MLP</td>\n",
              "      <td>0 days 00:00:11.545463</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>21</td>\n",
              "      <td>0.932740</td>\n",
              "      <td>MLP</td>\n",
              "      <td>0 days 00:00:12.920959</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2c11102a-ddb8-4b08-bff4-acb6e88b6243')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2c11102a-ddb8-4b08-bff4-acb6e88b6243 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2c11102a-ddb8-4b08-bff4-acb6e88b6243');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-7a395507-d9cd-4355-8a85-de6f4ba1fb9b\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7a395507-d9cd-4355-8a85-de6f4ba1fb9b')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-7a395507-d9cd-4355-8a85-de6f4ba1fb9b button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"        print(f\\\"Nie znaleziono badania o nazwie '{study_name}' w pliku \",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"number\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4,\n        \"min\": 10,\n        \"max\": 21,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          10,\n          21,\n          11\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"value\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0028496334319657846,\n        \"min\": 0.9327398538589478,\n        \"max\": 0.9396637082099915,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.9357072114944458,\n          0.9327398538589478,\n          0.9396637082099915\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"params_model_type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"MLP\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"duration\",\n      \"properties\": {\n        \"dtype\": \"timedelta64[ns]\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"0 days 00:00:13.536338\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_top5.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmgFIfCMkLFx",
        "outputId": "99a5cb48-c99a-4ce6-dc14-ea5bf4bbd175"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['number', 'value', 'datetime_start', 'datetime_complete', 'duration',\n",
              "       'params_batchSize', 'params_cnn_apply_dropout', 'params_cnn_dense_l0',\n",
              "       'params_cnn_dense_l1', 'params_cnn_dropout', 'params_cnn_filters_l0',\n",
              "       'params_cnn_filters_l1', 'params_cnn_filters_l2',\n",
              "       'params_cnn_kernel_size', 'params_cnn_n_conv_layers',\n",
              "       'params_cnn_n_dense_head', 'params_learningRate',\n",
              "       'params_mlp_apply_dropout', 'params_mlp_dropout', 'params_mlp_n_layers',\n",
              "       'params_mlp_units_l0', 'params_mlp_units_l1', 'params_mlp_units_l2',\n",
              "       'params_mlp_units_l3', 'params_model_type', 'params_optimizer',\n",
              "       'state'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "study.study_name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "EGEDo9AP1S3K",
        "outputId": "d8be2ab2-9307-4a4c-c566-5b528118b330"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Optuna_Hybrid_MLP_CNN_Time_v1'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Retrain Selected Model - W&B integration"
      ],
      "metadata": {
        "id": "ZtGH37KPkI1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === CONFIGURATION ===\n",
        "# Set to None to automatically select the best trial from the study.\n",
        "# Set to an integer (e.g., 5) to retrain a specific trial number.\n",
        "SELECTED_TRIAL_NUMBER = 13\n",
        "\n",
        "# How many epochs to train for the final run (usually more than in search)\n",
        "RETRAIN_EPOCHS = 10\n",
        "# =====================\n",
        "\n",
        "# 1. Retrieve parameters\n",
        "if SELECTED_TRIAL_NUMBER is None:\n",
        "    target_trial = study.best_trial\n",
        "    print(f\"Selected BEST trial (ID: {target_trial.number}) with val_acc: {target_trial.value:.4f}\")\n",
        "else:\n",
        "    # Find trial by number\n",
        "    target_trial = next(t for t in study.trials if t.number == SELECTED_TRIAL_NUMBER)\n",
        "    print(f\"Selected specific trial (ID: {target_trial.number}) with val_acc: {target_trial.value:.4f}\")\n",
        "\n",
        "\n",
        "optuna_params = target_trial.params\n",
        "print(\"Parameters:\", optuna_params)\n",
        "\n",
        "# Łączymy parametry z Optuny z Twoimi stałymi parametrami\n",
        "combined_config = {\n",
        "    \"retrain_epochs\": RETRAIN_EPOCHS,\n",
        "    \"trial_id\": target_trial.number,\n",
        "    \"optuna_original_test_acc\": target_trial.value,\n",
        "    # Nazwa badania, z którego pochodzi model\n",
        "    \"source_study_name\": study.study_name,\n",
        "    # Rozpakowanie parametrów z Optuny:\n",
        "    **optuna_params\n",
        "}\n",
        "\n",
        "# 1. Inicjalizacja W&B dla finalnego treningu\n",
        "run = wandb.init(\n",
        "    project=\"UAV-FDI-Optimization\",   # Ten sam projekt co wcześniej\n",
        "    group=\"Final_Training\",           # Nowa nazwa grupy (żeby oddzielić od searcha)\n",
        "    ## ----\n",
        "    name=\"Time_MLP_v1\",               # Unikalna nazwa tego konkretnego przebiegu\n",
        "    ## ----\n",
        "    config=combined_config,\n",
        "    reinit=True\n",
        ")\n",
        "\n",
        "print(\"W&B zinicjalizowane. Config:\", combined_config)\n",
        "\n",
        "# --- 3. Reconstruct Architecture & Build Model ---\n",
        "model_type = optuna_params[\"model_type\"]\n",
        "final_model = None # Zmieniam nazwę na final_model dla porządku\n",
        "\n",
        "if model_type == \"MLP\":\n",
        "    hidden_structure = []\n",
        "    n_layers = optuna_params[\"mlp_n_layers\"]\n",
        "    for i in range(n_layers):\n",
        "        hidden_structure.append(optuna_params[f\"mlp_units_l{i}\"])\n",
        "\n",
        "    print(f\"🏗️ Building MLP with structure: {hidden_structure}\")\n",
        "\n",
        "    final_model = MLPBuilder(\n",
        "        optimizerStr=optuna_params[\"optimizer\"],\n",
        "        dropout=optuna_params[\"mlp_dropout\"],\n",
        "        applyDropout=optuna_params[\"mlp_apply_dropout\"],\n",
        "        learningRate=optuna_params[\"learningRate\"],\n",
        "        hidden_layers_structure=hidden_structure\n",
        "    )\n",
        "\n",
        "elif model_type == \"CNN\":\n",
        "    conv_structure = []\n",
        "    n_conv = optuna_params[\"cnn_n_conv_layers\"]\n",
        "    for i in range(n_conv):\n",
        "        conv_structure.append(optuna_params[f\"cnn_filters_l{i}\"])\n",
        "\n",
        "    dense_structure = []\n",
        "    n_dense = optuna_params[\"cnn_n_dense_head\"]\n",
        "    for i in range(n_dense):\n",
        "        dense_structure.append(optuna_params[f\"cnn_dense_l{i}\"])\n",
        "\n",
        "    print(f\"🏗️ Building CNN with Conv: {conv_structure} and Dense: {dense_structure}\")\n",
        "\n",
        "    final_model = CNNBuilder(\n",
        "        optimizerStr=optuna_params[\"optimizer\"],\n",
        "        learningRate=optuna_params[\"learningRate\"],\n",
        "        conv_layers_structure=conv_structure,\n",
        "        kernel_size=optuna_params[\"cnn_kernel_size\"],\n",
        "        dense_layers_structure=dense_structure,\n",
        "        dropout=optuna_params[\"cnn_dropout\"],\n",
        "        applyDropout=optuna_params[\"cnn_apply_dropout\"]\n",
        "    )\n",
        "\n",
        "# 4. Prepare Data\n",
        "print(f\"Preparing data with Batch Size: {optuna_params['batchSize']}\")\n",
        "train_ds, val_ds, test_ds = CreateDataGenerators(optuna_params['batchSize'], X, y)\n",
        "\n",
        "# 5. Callbacks for Retraining\n",
        "callbacks_list = [\n",
        "    EarlyStopping(patience=15, restore_best_weights=True, monitor=\"val_accuracy\"),\n",
        "    ReduceLROnPlateau(factor=0.2, patience=10, min_lr=1e-6, monitor=\"val_loss\"),\n",
        "    WandbMetricsLogger(log_freq=\"epoch\")\n",
        "]\n",
        "\n",
        "\n",
        "# 6. Start Training\n",
        "print(f\"Rozpoczynam trening finalny ({RETRAIN_EPOCHS} epok)...\")\n",
        "\n",
        "history = final_model.fit(\n",
        "    train_ds,\n",
        "    epochs=RETRAIN_EPOCHS,\n",
        "    validation_data=val_ds,\n",
        "    callbacks=callbacks_list,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "\n",
        "# 7. Zapisz Model Checkpoint (Raz, po zakończeniu)\n",
        "save_folder = \"final_models_ckpt\"\n",
        "os.makedirs(save_folder, exist_ok=True)\n",
        "model_filename = f\"{optuna_params[\"model_type\"]}_{optuna_params[\"domain\"]}_cl{optuna_params[\"output_shape\"]}_trial_{target_trial.number}_from_{study.study_name}.keras\"\n",
        "checkpoint_path = os.path.join(save_folder, model_filename)\n",
        "\n",
        "final_model.save(checkpoint_path)\n",
        "print(f\"Model zapisany lokalnie jako: {model_filename}\")\n",
        "\n",
        "# Wrzuć plik modelu do chmury W&B\n",
        "wandb.save(model_filename)\n",
        "print(\"Model wysłany do Weights & Biases.\")\n",
        "\n",
        "# 4. Wygeneruj i wyślij Schemat architektury\n",
        "plot_filename = \"model_architecture.png\"\n",
        "try:\n",
        "    # Tworzymy plik graficzny ze schematem\n",
        "    plot_model(\n",
        "        final_model,\n",
        "        to_file=plot_filename,\n",
        "        show_shapes=True,\n",
        "        show_layer_names=True,\n",
        "        expand_nested=True\n",
        "    )\n",
        "\n",
        "    # Logujemy obrazek do dashboardu\n",
        "    wandb.log({\"model_chart\": wandb.Image(plot_filename)})\n",
        "    print(\"Schemat architektury (plot_model) wysłany do W&B.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Nie udało się wygenerować plot_model (może brakować graphviz): {e}\")\n",
        "\n",
        "# 5. Ewaluacja końcowa i zamknięcie\n",
        "loss, accuracy = final_model.evaluate(test_ds, verbose=0)\n",
        "wandb.log({\"test_accuracy\": accuracy, \"test_loss\": loss})\n",
        "\n",
        "\n",
        "print(f\"Wynik końcowy na teście: {accuracy:.4f}\")\n",
        "\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "Mi-0Hgw_w3Gu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrain Selected Model - without online logger"
      ],
      "metadata": {
        "id": "7B50tBinpGfC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === CONFIGURATION ===\n",
        "# Set to None to automatically select the best trial from the study.\n",
        "# Set to an integer (e.g., 5) to retrain a specific trial number.\n",
        "SELECTED_TRIAL_NUMBER = 10\n",
        "\n",
        "# How many epochs to train for the final run (usually more than in search)\n",
        "RETRAIN_EPOCHS = 100\n",
        "# =====================\n",
        "\n",
        "# 1. Retrieve parameters\n",
        "if SELECTED_TRIAL_NUMBER is None:\n",
        "    target_trial = study.best_trial\n",
        "    print(f\"Selected BEST trial (ID: {target_trial.number}) with val_acc: {target_trial.value:.4f}\")\n",
        "else:\n",
        "    # Find trial by number\n",
        "    target_trial = next(t for t in study.trials if t.number == SELECTED_TRIAL_NUMBER)\n",
        "    print(f\"Selected specific trial (ID: {target_trial.number}) with val_acc: {target_trial.value:.4f}\")\n",
        "\n",
        "params = target_trial.params\n",
        "print(\"Parameters:\", params)\n",
        "\n",
        "# 2. Reconstruct Architecture & Build Model\n",
        "model_type = params[\"model_type\"]\n",
        "model = None\n",
        "\n",
        "if model_type == \"MLP\":\n",
        "    # --- Reconstruct MLP Structure ---\n",
        "    hidden_structure = []\n",
        "    n_layers = params[\"mlp_n_layers\"]\n",
        "    for i in range(n_layers):\n",
        "        hidden_structure.append(params[f\"mlp_units_l{i}\"])\n",
        "\n",
        "    print(f\"Building MLP with structure: {hidden_structure}\")\n",
        "\n",
        "    model = MLPBuilder(\n",
        "        optimizerStr=params[\"optimizer\"],\n",
        "        dropout=params[\"mlp_dropout\"],\n",
        "        applyDropout=params[\"mlp_apply_dropout\"],\n",
        "        learningRate=params[\"learningRate\"],\n",
        "        hidden_layers_structure=hidden_structure\n",
        "    )\n",
        "\n",
        "elif model_type == \"CNN\":\n",
        "    # --- Reconstruct CNN Structure ---\n",
        "    conv_structure = []\n",
        "    n_conv = params[\"cnn_n_conv_layers\"]\n",
        "    for i in range(n_conv):\n",
        "        conv_structure.append(params[f\"cnn_filters_l{i}\"])\n",
        "\n",
        "    dense_structure = []\n",
        "    n_dense = params[\"cnn_n_dense_head\"]\n",
        "    for i in range(n_dense):\n",
        "        dense_structure.append(params[f\"cnn_dense_l{i}\"])\n",
        "\n",
        "    print(f\"Building CNN with Conv: {conv_structure} and Dense: {dense_structure}\")\n",
        "\n",
        "    model = CNNBuilder(\n",
        "        optimizerStr=params[\"optimizer\"],\n",
        "        learningRate=params[\"learningRate\"],\n",
        "        conv_layers_structure=conv_structure,\n",
        "        kernel_size=params[\"cnn_kernel_size\"],\n",
        "        dense_layers_structure=dense_structure,\n",
        "        dropout=params[\"cnn_dropout\"],\n",
        "        applyDropout=params[\"cnn_apply_Dropout\"]\n",
        "    )\n",
        "\n",
        "# 3. Prepare Data (using the trial's batch size)\n",
        "print(f\"Preparing data with Batch Size: {params['batchSize']}\")\n",
        "train_ds, val_ds, test_ds = CreateDataGenerators(params['batchSize'], X, y)\n",
        "\n",
        "# 4. Callbacks for Retraining\n",
        "\n",
        "save_folder = \"final_models_ckpt\"\n",
        "os.makedirs(save_folder, exist_ok=True)\n",
        "model_filename = f\"{params[\"model_type\"]}_{params[\"domain\"]}_cl{params[\"output_shape\"]}_trial_{target_trial.number}_from_{study.study_name}.keras\"\n",
        "checkpoint_path = os.path.join(save_folder, model_filename)\n",
        "\n",
        "callbacks_list = [\n",
        "    ModelCheckpoint(checkpoint_path, save_best_only=True, monitor=\"val_accuracy\", mode=\"max\", verbose=1),\n",
        "    EarlyStopping(patience=8, restore_best_weights=True, monitor=\"val_accuracy\"),\n",
        "    ReduceLROnPlateau(factor=0.2, patience=3, min_lr=1e-6, monitor=\"val_loss\")\n",
        "]\n",
        "\n",
        "# 5. Start Training\n",
        "print(f\"\\nStarting retraining for {RETRAIN_EPOCHS} epochs...\")\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    epochs=RETRAIN_EPOCHS,\n",
        "    validation_data=val_ds,\n",
        "    callbacks=callbacks_list,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 6. Final Evaluation\n",
        "print(\"\\n--- Final Evaluation on Test Set ---\")\n",
        "loss, accuracy = model.evaluate(test_ds)\n",
        "print(f\"Final Test Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Model saved to: {checkpoint_path}\")\n",
        "\n",
        "# 7. Plotting Results\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Accuracy Plot\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
        "plt.title(f'Accuracy (Trial {target_trial.number})')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Loss Plot\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "plt.title(f'Loss (Trial {target_trial.number})')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Bpjpxi81pGFM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}